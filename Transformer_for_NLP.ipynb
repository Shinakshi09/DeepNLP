{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer_for_NLP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6SFl33Pv9zG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "360657a0-ffe9-4266-eb85-f6c156f0a670"
      },
      "source": [
        "\"\"\" DataSet Description \"\"\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' DataSet Description '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvVesQU_IbUO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" Importing the libraries \"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "import re\n",
        "import time\n",
        "from google.colab import drive\n",
        "\n",
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except:\n",
        "    pass\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4tV-H7Bu_M3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" Data Pre-Processing :\n",
        "\n",
        "1. Loading the files\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "with open(\"/content/drive/My Drive/SelfPractise/DeepNLP/Transformer/europarl-v7.fr-en.en\",\n",
        "          mode='r',\n",
        "          encoding='utf-8') as f:\n",
        "    europarl_en = f.read()\n",
        "with open(\"/content/drive/My Drive/SelfPractise/DeepNLP/Transformer/europarl-v7.fr-en.fr\",\n",
        "          mode='r',\n",
        "          encoding='utf-8') as f:\n",
        "    europarl_fr = f.read()\n",
        "with open(\"/content/drive/My Drive/SelfPractise/DeepNLP/Transformer/P85-Non-Breaking-Prefix.en\",\n",
        "          mode='r',\n",
        "          encoding='utf-8') as f:\n",
        "    non_breaking_prefix_en = f.read()\n",
        "with open(\"/content/drive/My Drive/SelfPractise/DeepNLP/Transformer/P85-Non-Breaking-Prefix.fr\",\n",
        "          mode='r',\n",
        "          encoding='utf-8') as f:\n",
        "    non_breaking_prefix_fr = f.read()"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6qds-ENwBt2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\" 2. Cleaning the data \n",
        "\n",
        "I will split them in order to have the clean list of words and add space before each of them\n",
        "\n",
        "and dot after each of them because It will helps us to clean the data properly \"\"\"\n",
        "\n",
        "\n",
        "non_breaking_prefix_en = non_breaking_prefix_en.split(\"\\n\")\n",
        "non_breaking_prefix_en = [' ' + pref + '.' for pref in non_breaking_prefix_en]\n",
        "non_breaking_prefix_fr = non_breaking_prefix_fr.split(\"\\n\")\n",
        "non_breaking_prefix_fr = [' ' + pref + '.' for pref in non_breaking_prefix_fr]"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izGOhXyrxJun",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" 3. I will remove the points which are not full stop, I am removing unwnated points or dots to make it\n",
        "easier for our algorithm to understand what really is it, Let's say we got some sentence where we have class starts\n",
        "\n",
        "at 2 A.M in such cases, I will remove it and I will store it into a new corpus,\n",
        "\n",
        "I will write the function to find the dots or points which are not at the end of the sentence\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "corpus_en = europarl_en\n",
        "# Add $$$ after non ending sentence points\n",
        "for prefix in non_breaking_prefix_en:\n",
        "    corpus_en = corpus_en.replace(prefix, prefix + '$$$')\n",
        "\n",
        "    \"\"\" \n",
        "    Now I will check for the dots which are not followed by the space\n",
        "        If the full stop is followed by any number of letters then it is not considered Full stop \n",
        "        Here I will start with re.sub then type of string, We are looking for,\n",
        "        1. We will add points but will add backslash because point has a specific meaning in REGEX\n",
        "        2. Then Any sets of letters or number which is right after it and This one begins with ? = which means\n",
        "        it has to follow the points , Here = Means look for the letters or numbers but do not replace it\n",
        "        3. As I am looking for any number hence 0-9 \n",
        "        4. or , Please note , In REGEX , or is represented by | (vertical bar)\n",
        "        5. any letters small or capital hence written a-z | A-Z \n",
        "        6. But our aim is to replace the points only hence will mention here only consider the points for the\n",
        "           replacement part\n",
        "        7. Then at the end , I will specify the string, I would like to apply to, In my case here it is \n",
        "            corpus eng hence put that here \n",
        "            \n",
        "         NOw here we are I have selected all the points which i do not want to be ending points\n",
        "         Let's remove those points\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "corpus_en = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_en)\n",
        "\n",
        "\"\"\" Second regex is for removing these points or get replaced with nothing because we want to remove them\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "corpus_en = re.sub(r\".\\$\\$\\$\", '', corpus_en)\n",
        "\n",
        "\"\"\" Now There is posibility that There could be many white spaces \n",
        "1. I am saying to look for white space followed by white space + (LOok closely I have given two spaces) \n",
        "2. I am writing to say that I want to replace all those whites spaces with a single white space(Look I have given\n",
        "    one single white spaces and at the end our corpus which is corpus_En\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "corpus_en = re.sub(r\"  +\", \" \", corpus_en)\n",
        "\n",
        "\"\"\" NOw Let's split the corpus english with respect blacklash and n\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "corpus_en = corpus_en.split('\\n')\n",
        "\n",
        "\"\"\" Now We will do the same with french corpus too\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "corpus_fr = europarl_fr\n",
        "for prefix in non_breaking_prefix_fr:\n",
        "    corpus_fr = corpus_fr.replace(prefix, prefix + '$$$')\n",
        "corpus_fr = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_fr)\n",
        "corpus_fr = re.sub(r\".\\$\\$\\$\", '', corpus_fr)\n",
        "corpus_fr = re.sub(r\"  +\", \" \", corpus_fr)\n",
        "corpus_fr = corpus_fr.split('\\n')"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdylkjXXzmES",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" 4. Tokenizing the text, Tokenizing is the process of transforming a set of chracters into one corresponding\n",
        "\n",
        "number or token\n",
        "\n",
        "Here i will tfds library to tokenize the word \n",
        "\n",
        "Note : It is always advisible to have lower number of target_vocab_size, It helps in improving the performance\n",
        "\n",
        "of transformer\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "    corpus_en, target_vocab_size=2**13)\n",
        "\n",
        "\n",
        "tokenizer_fr = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "    corpus_fr, target_vocab_size=2**13)\n",
        "\n"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VG_OYebestx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" \n",
        "I will create two GLobal variable which I will use later\n",
        "1. Vocab size for English Language, \n",
        "\n",
        "I am giving value 2 here with plus sign because these are the words which will be added which means 2 words\n",
        "\n",
        "will be added but they are really words, They are just Homemade, This will be token which will be added at the\n",
        "\n",
        "begining of the sentence and other one will be added at the end of the sentence, As I said before, They are not\n",
        "\n",
        "really words but will be considered for the better of our model and Will do the same thing for french language\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "VOCAB_SIZE_EN = tokenizer_en.vocab_size + 2 # = 8190\n",
        "VOCAB_SIZE_FR = tokenizer_fr.vocab_size + 2 # = 8171\n",
        "\n",
        "\n",
        "\"\"\" \n",
        "Here I will create the real inputs and outputs to our model, Input will be a list of sentence, Each \n",
        "\n",
        "sentence being a sequence of numbers corressponding to the encoding of words and I will use starting and ending tokens\n",
        "so each sentence will start with the starting token that will be vocab size -2  which is exactly the vocab size of \n",
        "\n",
        "original tokenizer,\n",
        "\n",
        "Tokenizer_en_encode  is the string sentence into a list of numbers according to the tokenizing creation that\n",
        "\n",
        "I have already found. So I encode a sentence and add the end of our homemade ending token vocab size english-1\n",
        "\n",
        "and This is for every sentence in our original corpus,\n",
        "\n",
        "Now input is just a list of encoded sentences and will do the same with other language,\n",
        "\n",
        "Our second langauge is the OUTPUT Because We are building here a translator\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "inputs = [[VOCAB_SIZE_EN-2] + tokenizer_en.encode(sentence) + [VOCAB_SIZE_EN-1]\n",
        "          for sentence in corpus_en]\n",
        "outputs = [[VOCAB_SIZE_FR-2] + tokenizer_fr.encode(sentence) + [VOCAB_SIZE_FR-1]\n",
        "           for sentence in corpus_fr]"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DKNvT1_jMk6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" \n",
        "\n",
        "5. IF you will look at our dataset, It has some really long sentence and this is the issue here is why\n",
        "\n",
        "1. First one is when we will pads all sentences, So want to make all sentences the same lines for the batch to work\n",
        "\n",
        "2. It will take long time to train if we keep the really sentences and We reallu don't need to have those long\n",
        "\n",
        "sentences But IN case we are building a translator and have high power Machine, We can set the long sentences\n",
        "\n",
        "\n",
        "Here I am giving maximum length to the sentence as 20,\n",
        "\n",
        "Well! That seems quite low dataset\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "MAX_LENGTH = 20\n",
        "\n",
        "\"\"\" \n",
        "\n",
        "Creating a variable which will contain all the indices we want to get rid of \n",
        "\n",
        "Enumerate function is same as going through all the inputs but instead of just giving the sentences once at a time,\n",
        "\n",
        "it gives the sentences and counts so first element will zero and the sentence second sentences will give one and so on\n",
        "\n",
        "\"\"\"\n",
        "idx_to_remove = [count for count, sent in enumerate(inputs)\n",
        "                 if len(sent) > MAX_LENGTH]\n",
        "\n",
        "\"\"\" \n",
        "\n",
        "Here I am doing reverse element, Now let's undertand why am i doing reverse element, let's say we want to remove\n",
        "first and second element, Once we have removed second elements all the indices of the element after those one would be shifted\n",
        "by one because these one would be  will have disappeared hence we need to process in the reverse order so that we are removing right\n",
        "elements\n",
        "\n",
        "And there I have used dell function which allows us to remove elements according to their indices right here\n",
        "and I will do it for input and output because I want to match every elements in input with output\n",
        "\n",
        "OKay So now I Have removed all the sentences from english that were longer than 20 words\n",
        "and Will do the same for french too\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "for idx in reversed(idx_to_remove):\n",
        "    del inputs[idx]\n",
        "    del outputs[idx]\n",
        "idx_to_remove = [count for count, sent in enumerate(outputs)\n",
        "                 if len(sent) > MAX_LENGTH]\n",
        "for idx in reversed(idx_to_remove):\n",
        "    del inputs[idx]\n",
        "    del outputs[idx]\n",
        "\n"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbDW6B-gFp6Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" \n",
        "\n",
        "Input/ Output creation :\n",
        "\n",
        "\n",
        "1. We will pad all our input and output which means we want to make each sentence of same length \n",
        "\n",
        "2. Want to create dataset to shuffle back and do things which are neccessory\n",
        "\n",
        "I am padding POST which means I am padding zero at the end of each sentence  and Maxlength will tell How long\n",
        "\n",
        "our sentences will be \n",
        "\n",
        "Will do the same thing with output \n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
        "                                                       value=0,\n",
        "                                                       padding='post',\n",
        "                                                       maxlen=MAX_LENGTH)\n",
        "outputs = tf.keras.preprocessing.sequence.pad_sequences(outputs,\n",
        "                                                        value=0,\n",
        "                                                        padding='post',\n",
        "                                                        maxlen=MAX_LENGTH)\n",
        "\n",
        "\n",
        "\"\"\" \n",
        "\n",
        "Now I will create the Dataset\n",
        "\n",
        "Will decide the Batch_size, We want to use- I will take here 64 as our batch size, But later we can change it\n",
        "\n",
        "Shuffle siz = 20K\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 20000\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\n",
        "\n",
        "\"\"\" \n",
        "\n",
        "NOw WE have dataset but in order to make it fully usefull or optimize it  by our training phase, WIll use \n",
        "\n",
        "first dataset cache- This helps in improving the way data is stored, They way we can have access to the data\n",
        "\n",
        "during the training, It increases the speed of the training but has nothing to do with performance of the model\n",
        "\n",
        "NOte : It is not mandatory to use hence you can ignore it, \n",
        "\n",
        "Dataset.prefetch is also used to get the training faster\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "dataset = dataset.cache()\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2HDn6Z4JwmM",
        "colab_type": "text"
      },
      "source": [
        "# Positional Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5o_7aV3kKkom",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" \n",
        "\n",
        "This is very important part of Transformer , Now Will start building the transformer model\n",
        "\n",
        "I will not write here my own embedding because it is already available here But will create a customer layer\n",
        "\n",
        "in order to process this positional encoding that they explained in the Official paper , \n",
        "\n",
        "How positional encoding works , As I will be creating a custom layer hence it is going to be a class\n",
        "\n",
        "\n",
        "Here I am creating a class positionalencoding which will be inherit from layers.layer class\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class PositionalEncoding(layers.Layer):\n",
        "\n",
        "\n",
        "    \"\"\" \n",
        "    Whenever we are creating a custom layer or class we have to define a init method which is, each time you\n",
        "    create an object from this class, It will take it as an agurments \"Self\" just like any other method, \n",
        "    This is the layer which will make only mathematical computation hence won't need any more parameters \n",
        "    because there will no trainable weights, No sub layers or anything this kind at this position encoding phase\n",
        "    SO This init method will only call super positional encoding\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\" \n",
        "        \n",
        "        This line will call the init function of the class layer which has been well called the right there\n",
        "        so that the object can be built correctly with all the features that any layer can have\n",
        "\n",
        "        \"\"\"\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        \"\"\"\n",
        "        \n",
        "        Now I will create another method that will perform the operation which is inside the sin and cosine function\n",
        "        to understand that You can visit the official paper of Transformer \n",
        "        SO  here defining this method get_angel which accepts the parameters like\n",
        "        1. self\n",
        "        2. pos\n",
        "        3. i\n",
        "        4. d_model \n",
        "           Here I am just writing the formula which is used in transformer for positional encoding and will add it\n",
        "           to our input \n",
        "        \"\"\"\n",
        "    def get_angles(self, pos, i, d_model):\n",
        "        angles = 1 / np.power(10000., (2*(i//2)) / np.float32(d_model))\n",
        "        return pos * angles\n",
        "    def call(self, inputs):\n",
        "        seq_length = inputs.shape.as_list()[-2]\n",
        "        d_model = inputs.shape.as_list()[-1]\n",
        "        angles = self.get_angles(np.arange(seq_length)[:, np.newaxis],\n",
        "                                 np.arange(d_model)[np.newaxis, :],\n",
        "                                 d_model)\n",
        "        angles[:, 0::2] = np.sin(angles[:, 0::2])\n",
        "        angles[:, 1::2] = np.cos(angles[:, 1::2])\n",
        "        pos_encoding = angles[np.newaxis, ...]\n",
        "        return inputs + tf.cast(pos_encoding, tf.float32)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRkmjsV7T7jy",
        "colab_type": "text"
      },
      "source": [
        "# Attention Machanism "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKqLYBpiT0c4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" \n",
        "\n",
        "1. Scaled Dot product Attention  \n",
        "\n",
        "NOte : If you want to understand how am i writing this, GO through the Transformer Research Paper\n",
        "\n",
        "Here Mask's work is that it does not allow decoder to see the next word\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def scaled_dot_product_attention(queries, keys, values, mask):\n",
        "    product = tf.matmul(queries, keys, transpose_b=True)\n",
        "    \n",
        "    keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32)\n",
        "    scaled_product = product / tf.math.sqrt(keys_dim)\n",
        "    \n",
        "    if mask is not None:\n",
        "        scaled_product += (mask * -1e9)\n",
        "    \n",
        "    attention = tf.matmul(tf.nn.softmax(scaled_product, axis=-1), values)\n",
        "    \n",
        "    return attention"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2Hv5hpBiR5v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "Multi Head Attention Machanism\n",
        "\n",
        "Look at the architecture in the Research paper \n",
        "\n",
        "Here I will apply a linear function which is exactly a dense layer, Then SPlit the results into sub spaces\n",
        "\n",
        "For Each of those sub spaces, I Will apply the scale dot product attention And then I Will concatinate it \n",
        "\n",
        "so that we can get back to the words that have the right dimension which is the model\n",
        "\n",
        "And then Finally I will apply this last linear function to the whole computation\n",
        "\n",
        "Here I am writing a class which will be inherits from layers.layers And It will take init python function which will accept self,\n",
        "And Number of Projection we want\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "class MultiHeadAttention(layers.Layer):\n",
        "    \n",
        "    def __init__(self, nb_proj):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.nb_proj = nb_proj\n",
        "        \n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        assert self.d_model % self.nb_proj == 0\n",
        "        \n",
        "        self.d_proj = self.d_model // self.nb_proj\n",
        "        \n",
        "        self.query_lin = layers.Dense(units=self.d_model)\n",
        "        self.key_lin = layers.Dense(units=self.d_model)\n",
        "        self.value_lin = layers.Dense(units=self.d_model)\n",
        "        \n",
        "        self.final_lin = layers.Dense(units=self.d_model)\n",
        "        \n",
        "    def split_proj(self, inputs, batch_size): # inputs: (batch_size, seq_length, d_model)\n",
        "        shape = (batch_size,\n",
        "                 -1,\n",
        "                 self.nb_proj,\n",
        "                 self.d_proj)\n",
        "        splited_inputs = tf.reshape(inputs, shape=shape) # (batch_size, seq_length, nb_proj, d_proj)\n",
        "        return tf.transpose(splited_inputs, perm=[0, 2, 1, 3]) # (batch_size, nb_proj, seq_length, d_proj)\n",
        "    \n",
        "    def call(self, queries, keys, values, mask):\n",
        "        batch_size = tf.shape(queries)[0]\n",
        "        \n",
        "        queries = self.query_lin(queries)\n",
        "        keys = self.key_lin(keys)\n",
        "        values = self.value_lin(values)\n",
        "        \n",
        "        queries = self.split_proj(queries, batch_size)\n",
        "        keys = self.split_proj(keys, batch_size)\n",
        "        values = self.split_proj(values, batch_size)\n",
        "        \n",
        "        attention = scaled_dot_product_attention(queries, keys, values, mask)\n",
        "        \n",
        "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
        "        \n",
        "        concat_attention = tf.reshape(attention,\n",
        "                                      shape=(batch_size, -1, self.d_model))\n",
        "        \n",
        "        outputs = self.final_lin(concat_attention)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZLMoHp7KUkd",
        "colab_type": "text"
      },
      "source": [
        "# Encoder "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QApPdmGxKTsC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" \n",
        "\n",
        "Let's look at the plan or steps I Will be performing to build our first encoder\n",
        "\n",
        "1. Apply multi head attention\n",
        "\n",
        "2. And then Add the result to the input and\n",
        "\n",
        "3. Apply some regular linear function USING THE dense layer and again use these small add and known sequence at the end\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\"\"\" \n",
        "\n",
        "\n",
        "Now We will start by class EncoderLayer\n",
        "\n",
        "Then Define the __init__ function, self as usual \n",
        "\n",
        "And As input We will have is Feed Forward units, This Feed forward units are number of units which I will use in neural network\n",
        "\n",
        "Number of Projections : It is for the Attention machanism\n",
        "And then Dropout Rate that I will to control the overfitting \n",
        "\n",
        "then we call to Super which will be applied to encoder layer and self and call the init Method from it\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "class EncoderLayer(layers.Layer):\n",
        "    \n",
        "    def __init__(self, FFN_units, nb_proj, dropout_rate):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.FFN_units = FFN_units\n",
        "        self.nb_proj = nb_proj\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "\n",
        "\n",
        "        \"\"\" \n",
        "       \n",
        "            I will write the build method for dimension of the models and things when we create the object at encoder layer\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "\n",
        "        \"\"\" \n",
        "    \n",
        "        Here I am aplying multi head attention to the input with number of projection\n",
        "        Will add some Drop out and normalization layer with few Dense layer, \n",
        "        It is good to go through the Model's architecture while writing code for this\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        self.multi_head_attention = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        self.dense_1 = layers.Dense(units=self.FFN_units, activation=\"relu\")\n",
        "        self.dense_2 = layers.Dense(units=self.d_model)\n",
        "        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def call(self, inputs, mask, training):\n",
        "        attention = self.multi_head_attention(inputs,\n",
        "                                              inputs,\n",
        "                                              inputs,\n",
        "                                              mask)\n",
        "        attention = self.dropout_1(attention, training=training)\n",
        "        attention = self.norm_1(attention + inputs)\n",
        "        \n",
        "        outputs = self.dense_1(attention)\n",
        "        outputs = self.dense_2(outputs)\n",
        "        outputs = self.dropout_2(outputs, training=training)\n",
        "        outputs = self.norm_2(outputs + attention)\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjOFj438w116",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(layers.Layer):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 nb_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout_rate,\n",
        "                 vocab_size,\n",
        "                 d_model,\n",
        "                 name=\"encoder\"):\n",
        "        super(Encoder, self).__init__(name=name)\n",
        "        self.nb_layers = nb_layers\n",
        "        self.d_model = d_model\n",
        "        \n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "        self.enc_layers = [EncoderLayer(FFN_units,\n",
        "                                        nb_proj,\n",
        "                                        dropout_rate) \n",
        "                           for _ in range(nb_layers)]\n",
        "    \n",
        "    def call(self, inputs, mask, training):\n",
        "        outputs = self.embedding(inputs)\n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        outputs = self.pos_encoding(outputs)\n",
        "        outputs = self.dropout(outputs, training)\n",
        "        \n",
        "        for i in range(self.nb_layers):\n",
        "            outputs = self.enc_layers[i](outputs, mask, training)\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ra1iIZzozZYs",
        "colab_type": "text"
      },
      "source": [
        "# Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fswLIF1zXID",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        " \n",
        "In decoder part,  In decoder, Everything is same but only difference is that in Decoder, We use the Attention machanism twice\n",
        "\n",
        "Will start by creating the Sub layer which would be named Decoder sub layer which will have argument layers.layer\n",
        "\n",
        "\n",
        "and then use init function after a class as usual\n",
        "\n",
        "self,\n",
        "\n",
        "Feedforward units\n",
        "\n",
        "number of projections\n",
        "\n",
        "Dropout rates\n",
        "\n",
        "It is almost same as encoder not much difference \n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "class DecoderLayer(layers.Layer):\n",
        "    \n",
        "    def __init__(self, FFN_units, nb_proj, dropout_rate):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.FFN_units = FFN_units\n",
        "        self.nb_proj = nb_proj\n",
        "        self.dropout_rate = dropout_rate\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        \n",
        "        # Self multi head attention\n",
        "        self.multi_head_attention_1 = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        # Multi head attention combined with encoder output\n",
        "        self.multi_head_attention_2 = MultiHeadAttention(self.nb_proj)\n",
        "        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "        # Feed foward\n",
        "        self.dense_1 = layers.Dense(units=self.FFN_units,\n",
        "                                    activation=\"relu\")\n",
        "        self.dense_2 = layers.Dense(units=self.d_model)\n",
        "        self.dropout_3 = layers.Dropout(rate=self.dropout_rate)\n",
        "        self.norm_3 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        \n",
        "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "        attention = self.multi_head_attention_1(inputs,\n",
        "                                                inputs,\n",
        "                                                inputs,\n",
        "                                                mask_1)\n",
        "        attention = self.dropout_1(attention, training)\n",
        "        attention = self.norm_1(attention + inputs)\n",
        "        \n",
        "        attention_2 = self.multi_head_attention_2(attention,\n",
        "                                                  enc_outputs,\n",
        "                                                  enc_outputs,\n",
        "                                                  mask_2)\n",
        "        attention_2 = self.dropout_2(attention_2, training)\n",
        "        attention_2 = self.norm_2(attention_2 + attention)\n",
        "        \n",
        "        outputs = self.dense_1(attention_2)\n",
        "        outputs = self.dense_2(outputs)\n",
        "        outputs = self.dropout_3(outputs, training)\n",
        "        outputs = self.norm_3(outputs + attention_2)\n",
        "        \n",
        "        return outputs\n",
        "\n"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXBhR-N53FAF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(layers.Layer):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 nb_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout_rate,\n",
        "                 vocab_size,\n",
        "                 d_model,\n",
        "                 name=\"decoder\"):\n",
        "        super(Decoder, self).__init__(name=name)\n",
        "        self.d_model = d_model\n",
        "        self.nb_layers = nb_layers\n",
        "        \n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "        \n",
        "        self.dec_layers = [DecoderLayer(FFN_units,\n",
        "                                        nb_proj,\n",
        "                                        dropout_rate) \n",
        "                           for i in range(nb_layers)]\n",
        "    \n",
        "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "        outputs = self.embedding(inputs)\n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        outputs = self.pos_encoding(outputs)\n",
        "        outputs = self.dropout(outputs, training)\n",
        "        \n",
        "        for i in range(self.nb_layers):\n",
        "            outputs = self.dec_layers[i](outputs,\n",
        "                                         enc_outputs,\n",
        "                                         mask_1,\n",
        "                                         mask_2,\n",
        "                                         training)\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxP5TgH62YUv",
        "colab_type": "text"
      },
      "source": [
        "# Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQIpAREL2Qhn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "I have already made the parts of transformer\n",
        "\n",
        "Encoder and Decoder but few things were left there like Mask function within encoder \n",
        "\n",
        "and few things in decoder, Which I will be doing here\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class Transformer(tf.keras.Model):\n",
        "    \"\"\"\n",
        "\n",
        "    Vocab size is here because we need embeddings and this would be for encoder and other vocab size would be for decoder\n",
        "    d_model = which is dimension of the model which we will have at the end of the embedding\n",
        "    nb_layers = Number of layers, which means number of times we will apply the encoder and decoder layers, In the paper they said Six but I will try with other numbers too\n",
        "\n",
        "    FFN_Units = number of units inside Feed forward network\n",
        "\n",
        "    nb_proj = number of projections inside multihead attention layer\n",
        "    dropout_rate = to control overfitting\n",
        "    name = will give name to the model\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 vocab_size_enc,\n",
        "                 vocab_size_dec,\n",
        "                 d_model,\n",
        "                 nb_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout_rate,\n",
        "                 name=\"transformer\"):\n",
        "        super(Transformer, self).__init__(name=name)\n",
        "        \n",
        "        self.encoder = Encoder(nb_layers,\n",
        "                               FFN_units,\n",
        "                               nb_proj,\n",
        "                               dropout_rate,\n",
        "                               vocab_size_enc,\n",
        "                               d_model)\n",
        "        self.decoder = Decoder(nb_layers,\n",
        "                               FFN_units,\n",
        "                               nb_proj,\n",
        "                               dropout_rate,\n",
        "                               vocab_size_dec,\n",
        "                               d_model)\n",
        "        self.last_linear = layers.Dense(units=vocab_size_dec, name=\"lin_ouput\")\n",
        "\n",
        "\n",
        "        \"\"\" \n",
        "\n",
        "        \n",
        "          Now,This part is all about dealing with keys, values or queries in the matrix. We can have a look at\n",
        "          the research paper of the transformer . I Will create the masking function which will accept the sequence of input (dataset), \n",
        "          here tf.newaxis is just changing to the new Dimension, And Now Why did I do that, Look back at previous code while\n",
        "\n",
        "          creating the multihead attention machanism, I Had split the dimensions of the embedding which means that we have\n",
        "\n",
        "          we have an additional dimension, Look at the number of projections of the attention machanism, Now creating this\n",
        "\n",
        "          New dimension so that it can be applied to each of the projections \n",
        "\n",
        "        \"\"\"\n",
        "    \n",
        "    def create_padding_mask(self, seq):\n",
        "        mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "        return mask[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "             NOW, I will be creating not just a vector but a complete full matrix, and The Goal of this matrix is to \n",
        "             prevent it seeing the future words and best way to do is to use Triangular matrix is use this method from \n",
        "             tensorflow which is linalg.band_part. This Tringular matrix is only half top right part of the matrix, It is\n",
        "             in a tringular shape and does not have zeros\n",
        "    \n",
        "             This function will have self and seq as parameters then shape of the sequence as statement And As I have already\n",
        "            mentioned above, Will be creating a tringular matrix using this linal.band_part tensorflow method so what I am \n",
        "             doing here is adding the zero at the top right parts of this matrix. This function linal.band_part add condition\n",
        "             on the indices of all matrix and put zeros if those conditions are completed\n",
        "    \n",
        "             So Basically the first part says if the I in row and J in columns in all matrix, This add the condition about\n",
        "             how I supposed to  be higher than J\n",
        "    \n",
        "             Let me explain this I and J here. SO word I will not have access to word J, I want to mask the word that will be\n",
        "    \n",
        "             in the future and that is the reason I am 1- the whole matrix(means masking the future words here)\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "    def create_look_ahead_mask(self, seq):\n",
        "        seq_len = tf.shape(seq)[1]\n",
        "        look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "        return look_ahead_mask\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "            Here I will combine the encoder and decoder in the Call method , We get input for the encoder and input for\n",
        "            the decoder and information about whether we are training it or not\n",
        "    \n",
        "            And Will create the mask in the encoder, This will be a simple padding mask because Here transformer really dont\n",
        "            need look ahead mask\n",
        "        \"\"\"\n",
        "    \n",
        "    def call(self, enc_inputs, dec_inputs, training):\n",
        "        enc_mask = self.create_padding_mask(enc_inputs)\n",
        "        dec_mask_1 = tf.maximum(\n",
        "            self.create_padding_mask(dec_inputs),\n",
        "            self.create_look_ahead_mask(dec_inputs)\n",
        "        )\n",
        "        dec_mask_2 = self.create_padding_mask(enc_inputs)\n",
        "        \n",
        "        enc_outputs = self.encoder(enc_inputs, enc_mask, training)\n",
        "        dec_outputs = self.decoder(dec_inputs,\n",
        "                                   enc_outputs,\n",
        "                                   dec_mask_1,\n",
        "                                   dec_mask_2,\n",
        "                                   training)\n",
        "        \n",
        "        outputs = self.last_linear(dec_outputs)\n",
        "        \n",
        "        return outputs\n"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVs1CG6v6rhs",
        "colab_type": "text"
      },
      "source": [
        "# Training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TasEwpqc6iGA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "Will creat the model, and defining the hyper parameters\n",
        "\n",
        "I will have explained almost every parameters before hence won't be writing again\n",
        "\n",
        "\n",
        "NB_Layers should be as lower as possible because it makes your translator perfect\n",
        "\n",
        "\n",
        "Number_Projection : It is number of head\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Hyper-parameters\n",
        "D_MODEL = 128 # 512\n",
        "NB_LAYERS = 4 # 6\n",
        "FFN_UNITS = 512 # 2048\n",
        "NB_PROJ = 8 # 8\n",
        "DROPOUT_RATE = 0.1 # 0.1\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "Let's build the transformer\n",
        "Will put the hyper parameters there\n",
        "\n",
        "\"\"\"\n",
        "transformer = Transformer(vocab_size_enc=VOCAB_SIZE_EN,\n",
        "                          vocab_size_dec=VOCAB_SIZE_FR,\n",
        "                          d_model=D_MODEL,\n",
        "                          nb_layers=NB_LAYERS,\n",
        "                          FFN_units=FFN_UNITS,\n",
        "                          nb_proj=NB_PROJ,\n",
        "                          dropout_rate=DROPOUT_RATE)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwDCC2P-7jeS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "At this step, Will define the loss function \n",
        "\n",
        "\"\"\"\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                            reduction=\"none\")\n",
        "\n",
        "def loss_function(target, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(target, 0))\n",
        "    loss_ = loss_object(target, pred)\n",
        "    \n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    \n",
        "    return tf.reduce_mean(loss_)\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEEjVjBZ8BdT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" \n",
        "Designing the Optimizer part\n",
        "Designing my own learning rate\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    \n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "        \n",
        "        self.d_model = tf.cast(d_model, tf.float32)\n",
        "        self.warmup_steps = warmup_steps\n",
        "    \n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps**-1.5)\n",
        "        \n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "leaning_rate = CustomSchedule(D_MODEL)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(leaning_rate,\n",
        "                                     beta_1=0.9,\n",
        "                                     beta_2=0.98,\n",
        "                                     epsilon=1e-9)\n",
        "        "
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkurO-OO8wmX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" \n",
        "\n",
        "This check point is important in starting the model at every epochs or whatever number we can give to start the model from\n",
        "\n",
        "Will save it in my same google drive which I mounted\n",
        "\n",
        "WIll create a checkpoint manager , Work of Checkpoint manager is that\n",
        "\n",
        "If we have 5 check points, Can manage the check points, TO say if we have given maximum 5 check points. it can start\n",
        "\n",
        "new checkpoints after 5 \n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "checkpoint_path = \"./content/drive/My Drive/SelfPractise/DeepNLP/Transformer/ckpt/\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Latest checkpoint restored!!\")"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZOcrg19-1tJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1ab722db-727c-442f-bbb0-866ed935295e"
      },
      "source": [
        "\n",
        "EPOCHS = 10\n",
        "for epoch in range(EPOCHS):\n",
        "    print(\"Start of epoch {}\".format(epoch+1))\n",
        "    start = time.time()\n",
        "    \n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    \n",
        "    for (batch, (enc_inputs, targets)) in enumerate(dataset):\n",
        "        dec_inputs = targets[:, :-1]\n",
        "        dec_outputs_real = targets[:, 1:]\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = transformer(enc_inputs, dec_inputs, True)\n",
        "            loss = loss_function(dec_outputs_real, predictions)\n",
        "        \n",
        "        gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "        \n",
        "        train_loss(loss)\n",
        "        train_accuracy(dec_outputs_real, predictions)\n",
        "        \n",
        "        if batch % 50 == 0:\n",
        "            print(\"Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}\".format(\n",
        "                epoch+1, batch, train_loss.result(), train_accuracy.result()))\n",
        "            \n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print(\"Saving checkpoint for epoch {} at {}\".format(epoch+1,\n",
        "                                                        ckpt_save_path))\n",
        "    print(\"Time taken for 1 epoch: {} secs\\n\".format(time.time() - start))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start of epoch 1\n",
            "Epoch 1 Batch 0 Loss 6.3560 Accuracy 0.0000\n",
            "Epoch 1 Batch 50 Loss 6.2348 Accuracy 0.0051\n",
            "Epoch 1 Batch 100 Loss 6.2013 Accuracy 0.0269\n",
            "Epoch 1 Batch 150 Loss 6.1185 Accuracy 0.0338\n",
            "Epoch 1 Batch 200 Loss 6.0337 Accuracy 0.0367\n",
            "Epoch 1 Batch 250 Loss 5.9359 Accuracy 0.0434\n",
            "Epoch 1 Batch 300 Loss 5.8091 Accuracy 0.0511\n",
            "Epoch 1 Batch 350 Loss 5.6797 Accuracy 0.0571\n",
            "Epoch 1 Batch 400 Loss 5.5571 Accuracy 0.0616\n",
            "Epoch 1 Batch 450 Loss 5.4353 Accuracy 0.0654\n",
            "Epoch 1 Batch 500 Loss 5.3237 Accuracy 0.0703\n",
            "Epoch 1 Batch 550 Loss 5.2190 Accuracy 0.0755\n",
            "Epoch 1 Batch 600 Loss 5.1232 Accuracy 0.0808\n",
            "Epoch 1 Batch 650 Loss 5.0313 Accuracy 0.0858\n",
            "Epoch 1 Batch 700 Loss 4.9454 Accuracy 0.0907\n",
            "Epoch 1 Batch 750 Loss 4.8638 Accuracy 0.0958\n",
            "Epoch 1 Batch 800 Loss 4.7822 Accuracy 0.1005\n",
            "Epoch 1 Batch 850 Loss 4.7079 Accuracy 0.1054\n",
            "Epoch 1 Batch 900 Loss 4.6353 Accuracy 0.1101\n",
            "Epoch 1 Batch 950 Loss 4.5656 Accuracy 0.1146\n",
            "Epoch 1 Batch 1000 Loss 4.5001 Accuracy 0.1188\n",
            "Epoch 1 Batch 1050 Loss 4.4388 Accuracy 0.1229\n",
            "Epoch 1 Batch 1100 Loss 4.3818 Accuracy 0.1266\n",
            "Epoch 1 Batch 1150 Loss 4.3286 Accuracy 0.1302\n",
            "Epoch 1 Batch 1200 Loss 4.2764 Accuracy 0.1336\n",
            "Epoch 1 Batch 1250 Loss 4.2280 Accuracy 0.1367\n",
            "Epoch 1 Batch 1300 Loss 4.1806 Accuracy 0.1399\n",
            "Epoch 1 Batch 1350 Loss 4.1366 Accuracy 0.1430\n",
            "Epoch 1 Batch 1400 Loss 4.0950 Accuracy 0.1462\n",
            "Epoch 1 Batch 1450 Loss 4.0537 Accuracy 0.1492\n",
            "Epoch 1 Batch 1500 Loss 4.0148 Accuracy 0.1524\n",
            "Epoch 1 Batch 1550 Loss 3.9783 Accuracy 0.1553\n",
            "Epoch 1 Batch 1600 Loss 3.9438 Accuracy 0.1583\n",
            "Epoch 1 Batch 1650 Loss 3.9097 Accuracy 0.1611\n",
            "Epoch 1 Batch 1700 Loss 3.8759 Accuracy 0.1638\n",
            "Epoch 1 Batch 1750 Loss 3.8432 Accuracy 0.1665\n",
            "Epoch 1 Batch 1800 Loss 3.8115 Accuracy 0.1691\n",
            "Epoch 1 Batch 1850 Loss 3.7815 Accuracy 0.1717\n",
            "Epoch 1 Batch 1900 Loss 3.7528 Accuracy 0.1742\n",
            "Epoch 1 Batch 1950 Loss 3.7250 Accuracy 0.1766\n",
            "Epoch 1 Batch 2000 Loss 3.6980 Accuracy 0.1788\n",
            "Epoch 1 Batch 2050 Loss 3.6707 Accuracy 0.1810\n",
            "Epoch 1 Batch 2100 Loss 3.6444 Accuracy 0.1829\n",
            "Epoch 1 Batch 2150 Loss 3.6184 Accuracy 0.1848\n",
            "Epoch 1 Batch 2200 Loss 3.5923 Accuracy 0.1867\n",
            "Epoch 1 Batch 2250 Loss 3.5666 Accuracy 0.1885\n",
            "Epoch 1 Batch 2300 Loss 3.5416 Accuracy 0.1903\n",
            "Epoch 1 Batch 2350 Loss 3.5164 Accuracy 0.1921\n",
            "Epoch 1 Batch 2400 Loss 3.4934 Accuracy 0.1939\n",
            "Epoch 1 Batch 2450 Loss 3.4697 Accuracy 0.1956\n",
            "Epoch 1 Batch 2500 Loss 3.4471 Accuracy 0.1974\n",
            "Epoch 1 Batch 2550 Loss 3.4244 Accuracy 0.1992\n",
            "Epoch 1 Batch 2600 Loss 3.4022 Accuracy 0.2010\n",
            "Epoch 1 Batch 2650 Loss 3.3809 Accuracy 0.2029\n",
            "Epoch 1 Batch 2700 Loss 3.3598 Accuracy 0.2047\n",
            "Epoch 1 Batch 2750 Loss 3.3386 Accuracy 0.2065\n",
            "Epoch 1 Batch 2800 Loss 3.3180 Accuracy 0.2083\n",
            "Epoch 1 Batch 2850 Loss 3.2988 Accuracy 0.2101\n",
            "Epoch 1 Batch 2900 Loss 3.2789 Accuracy 0.2119\n",
            "Epoch 1 Batch 2950 Loss 3.2597 Accuracy 0.2136\n",
            "Epoch 1 Batch 3000 Loss 3.2406 Accuracy 0.2153\n",
            "Epoch 1 Batch 3050 Loss 3.2225 Accuracy 0.2171\n",
            "Epoch 1 Batch 3100 Loss 3.2044 Accuracy 0.2188\n",
            "Epoch 1 Batch 3150 Loss 3.1864 Accuracy 0.2205\n",
            "Epoch 1 Batch 3200 Loss 3.1689 Accuracy 0.2221\n",
            "Epoch 1 Batch 3250 Loss 3.1512 Accuracy 0.2238\n",
            "Epoch 1 Batch 3300 Loss 3.1335 Accuracy 0.2256\n",
            "Epoch 1 Batch 3350 Loss 3.1161 Accuracy 0.2272\n",
            "Epoch 1 Batch 3400 Loss 3.0996 Accuracy 0.2288\n",
            "Epoch 1 Batch 3450 Loss 3.0831 Accuracy 0.2306\n",
            "Epoch 1 Batch 3500 Loss 3.0672 Accuracy 0.2323\n",
            "Epoch 1 Batch 3550 Loss 3.0513 Accuracy 0.2339\n",
            "Epoch 1 Batch 3600 Loss 3.0356 Accuracy 0.2356\n",
            "Epoch 1 Batch 3650 Loss 3.0198 Accuracy 0.2373\n",
            "Epoch 1 Batch 3700 Loss 3.0038 Accuracy 0.2389\n",
            "Epoch 1 Batch 3750 Loss 2.9885 Accuracy 0.2406\n",
            "Epoch 1 Batch 3800 Loss 2.9737 Accuracy 0.2422\n",
            "Epoch 1 Batch 3850 Loss 2.9591 Accuracy 0.2440\n",
            "Epoch 1 Batch 3900 Loss 2.9449 Accuracy 0.2456\n",
            "Epoch 1 Batch 3950 Loss 2.9305 Accuracy 0.2472\n",
            "Epoch 1 Batch 4000 Loss 2.9163 Accuracy 0.2489\n",
            "Epoch 1 Batch 4050 Loss 2.9024 Accuracy 0.2504\n",
            "Epoch 1 Batch 4100 Loss 2.8887 Accuracy 0.2520\n",
            "Epoch 1 Batch 4150 Loss 2.8759 Accuracy 0.2534\n",
            "Epoch 1 Batch 4200 Loss 2.8639 Accuracy 0.2548\n",
            "Epoch 1 Batch 4250 Loss 2.8521 Accuracy 0.2561\n",
            "Epoch 1 Batch 4300 Loss 2.8407 Accuracy 0.2574\n",
            "Epoch 1 Batch 4350 Loss 2.8299 Accuracy 0.2586\n",
            "Epoch 1 Batch 4400 Loss 2.8192 Accuracy 0.2598\n",
            "Epoch 1 Batch 4450 Loss 2.8085 Accuracy 0.2608\n",
            "Epoch 1 Batch 4500 Loss 2.7983 Accuracy 0.2619\n",
            "Epoch 1 Batch 4550 Loss 2.7880 Accuracy 0.2631\n",
            "Epoch 1 Batch 4600 Loss 2.7780 Accuracy 0.2641\n",
            "Epoch 1 Batch 4650 Loss 2.7683 Accuracy 0.2652\n",
            "Epoch 1 Batch 4700 Loss 2.7591 Accuracy 0.2662\n",
            "Epoch 1 Batch 4750 Loss 2.7493 Accuracy 0.2672\n",
            "Epoch 1 Batch 4800 Loss 2.7397 Accuracy 0.2683\n",
            "Epoch 1 Batch 4850 Loss 2.7300 Accuracy 0.2694\n",
            "Epoch 1 Batch 4900 Loss 2.7205 Accuracy 0.2705\n",
            "Epoch 1 Batch 4950 Loss 2.7117 Accuracy 0.2715\n",
            "Epoch 1 Batch 5000 Loss 2.7028 Accuracy 0.2725\n",
            "Epoch 1 Batch 5050 Loss 2.6938 Accuracy 0.2735\n",
            "Epoch 1 Batch 5100 Loss 2.6848 Accuracy 0.2744\n",
            "Epoch 1 Batch 5150 Loss 2.6761 Accuracy 0.2753\n",
            "Epoch 1 Batch 5200 Loss 2.6675 Accuracy 0.2763\n",
            "Epoch 1 Batch 5250 Loss 2.6591 Accuracy 0.2771\n",
            "Epoch 1 Batch 5300 Loss 2.6503 Accuracy 0.2780\n",
            "Epoch 1 Batch 5350 Loss 2.6420 Accuracy 0.2789\n",
            "Epoch 1 Batch 5400 Loss 2.6332 Accuracy 0.2797\n",
            "Epoch 1 Batch 5450 Loss 2.6250 Accuracy 0.2806\n",
            "Epoch 1 Batch 5500 Loss 2.6165 Accuracy 0.2814\n",
            "Epoch 1 Batch 5550 Loss 2.6078 Accuracy 0.2822\n",
            "Epoch 1 Batch 5600 Loss 2.5993 Accuracy 0.2831\n",
            "Epoch 1 Batch 5650 Loss 2.5912 Accuracy 0.2839\n",
            "Epoch 1 Batch 5700 Loss 2.5831 Accuracy 0.2848\n",
            "Saving checkpoint for epoch 1 at ./content/drive/My Drive/SelfPractise/DeepNLP/Transformer/ckpt/ckpt-1\n",
            "Time taken for 1 epoch: 1547.4630353450775 secs\n",
            "\n",
            "Start of epoch 2\n",
            "Epoch 2 Batch 0 Loss 1.6373 Accuracy 0.3939\n",
            "Epoch 2 Batch 50 Loss 1.7127 Accuracy 0.3842\n",
            "Epoch 2 Batch 100 Loss 1.7106 Accuracy 0.3846\n",
            "Epoch 2 Batch 150 Loss 1.7040 Accuracy 0.3854\n",
            "Epoch 2 Batch 200 Loss 1.6880 Accuracy 0.3870\n",
            "Epoch 2 Batch 250 Loss 1.6857 Accuracy 0.3876\n",
            "Epoch 2 Batch 300 Loss 1.6820 Accuracy 0.3885\n",
            "Epoch 2 Batch 350 Loss 1.6738 Accuracy 0.3892\n",
            "Epoch 2 Batch 400 Loss 1.6659 Accuracy 0.3897\n",
            "Epoch 2 Batch 450 Loss 1.6601 Accuracy 0.3896\n",
            "Epoch 2 Batch 500 Loss 1.6569 Accuracy 0.3897\n",
            "Epoch 2 Batch 550 Loss 1.6535 Accuracy 0.3899\n",
            "Epoch 2 Batch 600 Loss 1.6535 Accuracy 0.3901\n",
            "Epoch 2 Batch 650 Loss 1.6470 Accuracy 0.3908\n",
            "Epoch 2 Batch 700 Loss 1.6450 Accuracy 0.3914\n",
            "Epoch 2 Batch 750 Loss 1.6394 Accuracy 0.3920\n",
            "Epoch 2 Batch 800 Loss 1.6362 Accuracy 0.3926\n",
            "Epoch 2 Batch 850 Loss 1.6326 Accuracy 0.3930\n",
            "Epoch 2 Batch 900 Loss 1.6279 Accuracy 0.3932\n",
            "Epoch 2 Batch 950 Loss 1.6229 Accuracy 0.3938\n",
            "Epoch 2 Batch 1000 Loss 1.6186 Accuracy 0.3944\n",
            "Epoch 2 Batch 1050 Loss 1.6156 Accuracy 0.3948\n",
            "Epoch 2 Batch 1100 Loss 1.6123 Accuracy 0.3952\n",
            "Epoch 2 Batch 1150 Loss 1.6094 Accuracy 0.3952\n",
            "Epoch 2 Batch 1200 Loss 1.6071 Accuracy 0.3953\n",
            "Epoch 2 Batch 1250 Loss 1.6029 Accuracy 0.3958\n",
            "Epoch 2 Batch 1300 Loss 1.5987 Accuracy 0.3965\n",
            "Epoch 2 Batch 1350 Loss 1.5954 Accuracy 0.3973\n",
            "Epoch 2 Batch 1400 Loss 1.5923 Accuracy 0.3981\n",
            "Epoch 2 Batch 1450 Loss 1.5885 Accuracy 0.3989\n",
            "Epoch 2 Batch 1500 Loss 1.5838 Accuracy 0.3999\n",
            "Epoch 2 Batch 1550 Loss 1.5797 Accuracy 0.4008\n",
            "Epoch 2 Batch 1600 Loss 1.5755 Accuracy 0.4020\n",
            "Epoch 2 Batch 1650 Loss 1.5720 Accuracy 0.4029\n",
            "Epoch 2 Batch 1700 Loss 1.5690 Accuracy 0.4038\n",
            "Epoch 2 Batch 1750 Loss 1.5652 Accuracy 0.4047\n",
            "Epoch 2 Batch 1800 Loss 1.5618 Accuracy 0.4058\n",
            "Epoch 2 Batch 1850 Loss 1.5584 Accuracy 0.4067\n",
            "Epoch 2 Batch 1900 Loss 1.5548 Accuracy 0.4076\n",
            "Epoch 2 Batch 1950 Loss 1.5510 Accuracy 0.4087\n",
            "Epoch 2 Batch 2000 Loss 1.5460 Accuracy 0.4095\n",
            "Epoch 2 Batch 2050 Loss 1.5421 Accuracy 0.4102\n",
            "Epoch 2 Batch 2100 Loss 1.5375 Accuracy 0.4108\n",
            "Epoch 2 Batch 2150 Loss 1.5334 Accuracy 0.4113\n",
            "Epoch 2 Batch 2200 Loss 1.5276 Accuracy 0.4118\n",
            "Epoch 2 Batch 2250 Loss 1.5221 Accuracy 0.4123\n",
            "Epoch 2 Batch 2300 Loss 1.5170 Accuracy 0.4127\n",
            "Epoch 2 Batch 2350 Loss 1.5122 Accuracy 0.4131\n",
            "Epoch 2 Batch 2400 Loss 1.5073 Accuracy 0.4136\n",
            "Epoch 2 Batch 2450 Loss 1.5028 Accuracy 0.4141\n",
            "Epoch 2 Batch 2500 Loss 1.4976 Accuracy 0.4147\n",
            "Epoch 2 Batch 2550 Loss 1.4931 Accuracy 0.4152\n",
            "Epoch 2 Batch 2600 Loss 1.4886 Accuracy 0.4158\n",
            "Epoch 2 Batch 2650 Loss 1.4836 Accuracy 0.4164\n",
            "Epoch 2 Batch 2700 Loss 1.4794 Accuracy 0.4171\n",
            "Epoch 2 Batch 2750 Loss 1.4751 Accuracy 0.4177\n",
            "Epoch 2 Batch 2800 Loss 1.4713 Accuracy 0.4182\n",
            "Epoch 2 Batch 2850 Loss 1.4674 Accuracy 0.4187\n",
            "Epoch 2 Batch 2900 Loss 1.4633 Accuracy 0.4193\n",
            "Epoch 2 Batch 2950 Loss 1.4595 Accuracy 0.4198\n",
            "Epoch 2 Batch 3000 Loss 1.4560 Accuracy 0.4203\n",
            "Epoch 2 Batch 3050 Loss 1.4522 Accuracy 0.4208\n",
            "Epoch 2 Batch 3100 Loss 1.4488 Accuracy 0.4213\n",
            "Epoch 2 Batch 3150 Loss 1.4446 Accuracy 0.4220\n",
            "Epoch 2 Batch 3200 Loss 1.4407 Accuracy 0.4225\n",
            "Epoch 2 Batch 3250 Loss 1.4368 Accuracy 0.4231\n",
            "Epoch 2 Batch 3300 Loss 1.4331 Accuracy 0.4236\n",
            "Epoch 2 Batch 3350 Loss 1.4295 Accuracy 0.4241\n",
            "Epoch 2 Batch 3400 Loss 1.4262 Accuracy 0.4246\n",
            "Epoch 2 Batch 3450 Loss 1.4223 Accuracy 0.4251\n",
            "Epoch 2 Batch 3500 Loss 1.4188 Accuracy 0.4256\n",
            "Epoch 2 Batch 3550 Loss 1.4154 Accuracy 0.4261\n",
            "Epoch 2 Batch 3600 Loss 1.4120 Accuracy 0.4267\n",
            "Epoch 2 Batch 3650 Loss 1.4087 Accuracy 0.4273\n",
            "Epoch 2 Batch 3700 Loss 1.4052 Accuracy 0.4279\n",
            "Epoch 2 Batch 3750 Loss 1.4020 Accuracy 0.4284\n",
            "Epoch 2 Batch 3800 Loss 1.3988 Accuracy 0.4289\n",
            "Epoch 2 Batch 3850 Loss 1.3957 Accuracy 0.4295\n",
            "Epoch 2 Batch 3900 Loss 1.3926 Accuracy 0.4300\n",
            "Epoch 2 Batch 3950 Loss 1.3898 Accuracy 0.4305\n",
            "Epoch 2 Batch 4000 Loss 1.3869 Accuracy 0.4311\n",
            "Epoch 2 Batch 4050 Loss 1.3845 Accuracy 0.4316\n",
            "Epoch 2 Batch 4100 Loss 1.3821 Accuracy 0.4320\n",
            "Epoch 2 Batch 4150 Loss 1.3801 Accuracy 0.4324\n",
            "Epoch 2 Batch 4200 Loss 1.3787 Accuracy 0.4326\n",
            "Epoch 2 Batch 4250 Loss 1.3779 Accuracy 0.4328\n",
            "Epoch 2 Batch 4300 Loss 1.3773 Accuracy 0.4329\n",
            "Epoch 2 Batch 4350 Loss 1.3773 Accuracy 0.4330\n",
            "Epoch 2 Batch 4400 Loss 1.3773 Accuracy 0.4330\n",
            "Epoch 2 Batch 4450 Loss 1.3767 Accuracy 0.4330\n",
            "Epoch 2 Batch 4500 Loss 1.3765 Accuracy 0.4331\n",
            "Epoch 2 Batch 4550 Loss 1.3761 Accuracy 0.4331\n",
            "Epoch 2 Batch 4600 Loss 1.3760 Accuracy 0.4331\n",
            "Epoch 2 Batch 4650 Loss 1.3759 Accuracy 0.4332\n",
            "Epoch 2 Batch 4700 Loss 1.3762 Accuracy 0.4331\n",
            "Epoch 2 Batch 4750 Loss 1.3763 Accuracy 0.4332\n",
            "Epoch 2 Batch 4800 Loss 1.3762 Accuracy 0.4332\n",
            "Epoch 2 Batch 4850 Loss 1.3761 Accuracy 0.4333\n",
            "Epoch 2 Batch 4900 Loss 1.3760 Accuracy 0.4333\n",
            "Epoch 2 Batch 4950 Loss 1.3761 Accuracy 0.4333\n",
            "Epoch 2 Batch 5000 Loss 1.3763 Accuracy 0.4333\n",
            "Epoch 2 Batch 5050 Loss 1.3762 Accuracy 0.4333\n",
            "Epoch 2 Batch 5100 Loss 1.3764 Accuracy 0.4333\n",
            "Epoch 2 Batch 5150 Loss 1.3764 Accuracy 0.4333\n",
            "Epoch 2 Batch 5200 Loss 1.3763 Accuracy 0.4332\n",
            "Epoch 2 Batch 5250 Loss 1.3763 Accuracy 0.4331\n",
            "Epoch 2 Batch 5300 Loss 1.3762 Accuracy 0.4331\n",
            "Epoch 2 Batch 5350 Loss 1.3760 Accuracy 0.4330\n",
            "Epoch 2 Batch 5400 Loss 1.3756 Accuracy 0.4329\n",
            "Epoch 2 Batch 5450 Loss 1.3753 Accuracy 0.4329\n",
            "Epoch 2 Batch 5500 Loss 1.3749 Accuracy 0.4328\n",
            "Epoch 2 Batch 5550 Loss 1.3744 Accuracy 0.4328\n",
            "Epoch 2 Batch 5600 Loss 1.3742 Accuracy 0.4327\n",
            "Epoch 2 Batch 5650 Loss 1.3739 Accuracy 0.4327\n",
            "Epoch 2 Batch 5700 Loss 1.3734 Accuracy 0.4327\n",
            "Saving checkpoint for epoch 2 at ./content/drive/My Drive/SelfPractise/DeepNLP/Transformer/ckpt/ckpt-2\n",
            "Time taken for 1 epoch: 1532.8103420734406 secs\n",
            "\n",
            "Start of epoch 3\n",
            "Epoch 3 Batch 0 Loss 1.4493 Accuracy 0.4252\n",
            "Epoch 3 Batch 50 Loss 1.3638 Accuracy 0.4305\n",
            "Epoch 3 Batch 100 Loss 1.3390 Accuracy 0.4342\n",
            "Epoch 3 Batch 150 Loss 1.3439 Accuracy 0.4359\n",
            "Epoch 3 Batch 200 Loss 1.3355 Accuracy 0.4384\n",
            "Epoch 3 Batch 250 Loss 1.3303 Accuracy 0.4388\n",
            "Epoch 3 Batch 300 Loss 1.3287 Accuracy 0.4378\n",
            "Epoch 3 Batch 350 Loss 1.3266 Accuracy 0.4372\n",
            "Epoch 3 Batch 400 Loss 1.3220 Accuracy 0.4379\n",
            "Epoch 3 Batch 450 Loss 1.3217 Accuracy 0.4383\n",
            "Epoch 3 Batch 500 Loss 1.3200 Accuracy 0.4386\n",
            "Epoch 3 Batch 550 Loss 1.3181 Accuracy 0.4387\n",
            "Epoch 3 Batch 600 Loss 1.3165 Accuracy 0.4388\n",
            "Epoch 3 Batch 650 Loss 1.3149 Accuracy 0.4391\n",
            "Epoch 3 Batch 700 Loss 1.3163 Accuracy 0.4395\n",
            "Epoch 3 Batch 750 Loss 1.3131 Accuracy 0.4398\n",
            "Epoch 3 Batch 800 Loss 1.3138 Accuracy 0.4400\n",
            "Epoch 3 Batch 850 Loss 1.3117 Accuracy 0.4402\n",
            "Epoch 3 Batch 900 Loss 1.3105 Accuracy 0.4400\n",
            "Epoch 3 Batch 950 Loss 1.3079 Accuracy 0.4401\n",
            "Epoch 3 Batch 1000 Loss 1.3057 Accuracy 0.4402\n",
            "Epoch 3 Batch 1050 Loss 1.3050 Accuracy 0.4404\n",
            "Epoch 3 Batch 1100 Loss 1.3039 Accuracy 0.4405\n",
            "Epoch 3 Batch 1150 Loss 1.3009 Accuracy 0.4406\n",
            "Epoch 3 Batch 1200 Loss 1.2996 Accuracy 0.4408\n",
            "Epoch 3 Batch 1250 Loss 1.2974 Accuracy 0.4411\n",
            "Epoch 3 Batch 1300 Loss 1.2949 Accuracy 0.4416\n",
            "Epoch 3 Batch 1350 Loss 1.2921 Accuracy 0.4422\n",
            "Epoch 3 Batch 1400 Loss 1.2896 Accuracy 0.4429\n",
            "Epoch 3 Batch 1450 Loss 1.2871 Accuracy 0.4437\n",
            "Epoch 3 Batch 1500 Loss 1.2839 Accuracy 0.4446\n",
            "Epoch 3 Batch 1550 Loss 1.2811 Accuracy 0.4455\n",
            "Epoch 3 Batch 1600 Loss 1.2790 Accuracy 0.4463\n",
            "Epoch 3 Batch 1650 Loss 1.2757 Accuracy 0.4472\n",
            "Epoch 3 Batch 1700 Loss 1.2724 Accuracy 0.4480\n",
            "Epoch 3 Batch 1750 Loss 1.2700 Accuracy 0.4488\n",
            "Epoch 3 Batch 1800 Loss 1.2674 Accuracy 0.4496\n",
            "Epoch 3 Batch 1850 Loss 1.2642 Accuracy 0.4504\n",
            "Epoch 3 Batch 1900 Loss 1.2614 Accuracy 0.4512\n",
            "Epoch 3 Batch 1950 Loss 1.2595 Accuracy 0.4519\n",
            "Epoch 3 Batch 2000 Loss 1.2577 Accuracy 0.4526\n",
            "Epoch 3 Batch 2050 Loss 1.2553 Accuracy 0.4534\n",
            "Epoch 3 Batch 2100 Loss 1.2525 Accuracy 0.4537\n",
            "Epoch 3 Batch 2150 Loss 1.2490 Accuracy 0.4540\n",
            "Epoch 3 Batch 2200 Loss 1.2457 Accuracy 0.4542\n",
            "Epoch 3 Batch 2250 Loss 1.2421 Accuracy 0.4545\n",
            "Epoch 3 Batch 2300 Loss 1.2390 Accuracy 0.4547\n",
            "Epoch 3 Batch 2350 Loss 1.2352 Accuracy 0.4551\n",
            "Epoch 3 Batch 2400 Loss 1.2314 Accuracy 0.4555\n",
            "Epoch 3 Batch 2450 Loss 1.2278 Accuracy 0.4559\n",
            "Epoch 3 Batch 2500 Loss 1.2241 Accuracy 0.4562\n",
            "Epoch 3 Batch 2550 Loss 1.2211 Accuracy 0.4566\n",
            "Epoch 3 Batch 2600 Loss 1.2183 Accuracy 0.4570\n",
            "Epoch 3 Batch 2650 Loss 1.2155 Accuracy 0.4573\n",
            "Epoch 3 Batch 2700 Loss 1.2125 Accuracy 0.4577\n",
            "Epoch 3 Batch 2750 Loss 1.2096 Accuracy 0.4581\n",
            "Epoch 3 Batch 2800 Loss 1.2070 Accuracy 0.4586\n",
            "Epoch 3 Batch 2850 Loss 1.2041 Accuracy 0.4589\n",
            "Epoch 3 Batch 2900 Loss 1.2016 Accuracy 0.4591\n",
            "Epoch 3 Batch 2950 Loss 1.1989 Accuracy 0.4595\n",
            "Epoch 3 Batch 3000 Loss 1.1962 Accuracy 0.4599\n",
            "Epoch 3 Batch 3050 Loss 1.1936 Accuracy 0.4603\n",
            "Epoch 3 Batch 3100 Loss 1.1913 Accuracy 0.4606\n",
            "Epoch 3 Batch 3150 Loss 1.1887 Accuracy 0.4609\n",
            "Epoch 3 Batch 3200 Loss 1.1857 Accuracy 0.4612\n",
            "Epoch 3 Batch 3250 Loss 1.1830 Accuracy 0.4615\n",
            "Epoch 3 Batch 3300 Loss 1.1804 Accuracy 0.4619\n",
            "Epoch 3 Batch 3350 Loss 1.1780 Accuracy 0.4623\n",
            "Epoch 3 Batch 3400 Loss 1.1754 Accuracy 0.4626\n",
            "Epoch 3 Batch 3450 Loss 1.1731 Accuracy 0.4629\n",
            "Epoch 3 Batch 3500 Loss 1.1707 Accuracy 0.4633\n",
            "Epoch 3 Batch 3550 Loss 1.1686 Accuracy 0.4637\n",
            "Epoch 3 Batch 3600 Loss 1.1663 Accuracy 0.4640\n",
            "Epoch 3 Batch 3650 Loss 1.1639 Accuracy 0.4644\n",
            "Epoch 3 Batch 3700 Loss 1.1618 Accuracy 0.4648\n",
            "Epoch 3 Batch 3750 Loss 1.1596 Accuracy 0.4652\n",
            "Epoch 3 Batch 3800 Loss 1.1576 Accuracy 0.4657\n",
            "Epoch 3 Batch 3850 Loss 1.1559 Accuracy 0.4661\n",
            "Epoch 3 Batch 3900 Loss 1.1541 Accuracy 0.4666\n",
            "Epoch 3 Batch 3950 Loss 1.1521 Accuracy 0.4670\n",
            "Epoch 3 Batch 4000 Loss 1.1503 Accuracy 0.4674\n",
            "Epoch 3 Batch 4050 Loss 1.1485 Accuracy 0.4677\n",
            "Epoch 3 Batch 4100 Loss 1.1470 Accuracy 0.4681\n",
            "Epoch 3 Batch 4150 Loss 1.1462 Accuracy 0.4682\n",
            "Epoch 3 Batch 4200 Loss 1.1462 Accuracy 0.4683\n",
            "Epoch 3 Batch 4250 Loss 1.1459 Accuracy 0.4683\n",
            "Epoch 3 Batch 4300 Loss 1.1464 Accuracy 0.4684\n",
            "Epoch 3 Batch 4350 Loss 1.1468 Accuracy 0.4683\n",
            "Epoch 3 Batch 4400 Loss 1.1475 Accuracy 0.4683\n",
            "Epoch 3 Batch 4450 Loss 1.1485 Accuracy 0.4681\n",
            "Epoch 3 Batch 4500 Loss 1.1495 Accuracy 0.4679\n",
            "Epoch 3 Batch 4550 Loss 1.1503 Accuracy 0.4678\n",
            "Epoch 3 Batch 4600 Loss 1.1513 Accuracy 0.4676\n",
            "Epoch 3 Batch 4650 Loss 1.1521 Accuracy 0.4675\n",
            "Epoch 3 Batch 4700 Loss 1.1531 Accuracy 0.4674\n",
            "Epoch 3 Batch 4750 Loss 1.1541 Accuracy 0.4672\n",
            "Epoch 3 Batch 4800 Loss 1.1550 Accuracy 0.4671\n",
            "Epoch 3 Batch 4850 Loss 1.1555 Accuracy 0.4670\n",
            "Epoch 3 Batch 4900 Loss 1.1567 Accuracy 0.4669\n",
            "Epoch 3 Batch 4950 Loss 1.1576 Accuracy 0.4667\n",
            "Epoch 3 Batch 5000 Loss 1.1585 Accuracy 0.4666\n",
            "Epoch 3 Batch 5050 Loss 1.1593 Accuracy 0.4665\n",
            "Epoch 3 Batch 5100 Loss 1.1598 Accuracy 0.4664\n",
            "Epoch 3 Batch 5150 Loss 1.1607 Accuracy 0.4662\n",
            "Epoch 3 Batch 5200 Loss 1.1616 Accuracy 0.4660\n",
            "Epoch 3 Batch 5250 Loss 1.1625 Accuracy 0.4658\n",
            "Epoch 3 Batch 5300 Loss 1.1633 Accuracy 0.4655\n",
            "Epoch 3 Batch 5350 Loss 1.1639 Accuracy 0.4653\n",
            "Epoch 3 Batch 5400 Loss 1.1647 Accuracy 0.4651\n",
            "Epoch 3 Batch 5450 Loss 1.1653 Accuracy 0.4649\n",
            "Epoch 3 Batch 5500 Loss 1.1658 Accuracy 0.4647\n",
            "Epoch 3 Batch 5550 Loss 1.1662 Accuracy 0.4646\n",
            "Epoch 3 Batch 5600 Loss 1.1666 Accuracy 0.4644\n",
            "Epoch 3 Batch 5650 Loss 1.1672 Accuracy 0.4642\n",
            "Epoch 3 Batch 5700 Loss 1.1679 Accuracy 0.4641\n",
            "Saving checkpoint for epoch 3 at ./content/drive/My Drive/SelfPractise/DeepNLP/Transformer/ckpt/ckpt-3\n",
            "Time taken for 1 epoch: 1534.1576981544495 secs\n",
            "\n",
            "Start of epoch 4\n",
            "Epoch 4 Batch 0 Loss 1.1703 Accuracy 0.4663\n",
            "Epoch 4 Batch 50 Loss 1.2074 Accuracy 0.4582\n",
            "Epoch 4 Batch 100 Loss 1.2142 Accuracy 0.4591\n",
            "Epoch 4 Batch 150 Loss 1.2155 Accuracy 0.4586\n",
            "Epoch 4 Batch 200 Loss 1.2157 Accuracy 0.4579\n",
            "Epoch 4 Batch 250 Loss 1.2135 Accuracy 0.4573\n",
            "Epoch 4 Batch 300 Loss 1.2164 Accuracy 0.4577\n",
            "Epoch 4 Batch 350 Loss 1.2147 Accuracy 0.4573\n",
            "Epoch 4 Batch 400 Loss 1.2110 Accuracy 0.4567\n",
            "Epoch 4 Batch 450 Loss 1.2060 Accuracy 0.4570\n",
            "Epoch 4 Batch 500 Loss 1.2064 Accuracy 0.4566\n",
            "Epoch 4 Batch 550 Loss 1.2055 Accuracy 0.4565\n",
            "Epoch 4 Batch 600 Loss 1.2057 Accuracy 0.4562\n",
            "Epoch 4 Batch 650 Loss 1.2049 Accuracy 0.4569\n",
            "Epoch 4 Batch 700 Loss 1.2041 Accuracy 0.4569\n",
            "Epoch 4 Batch 750 Loss 1.2039 Accuracy 0.4571\n",
            "Epoch 4 Batch 800 Loss 1.2025 Accuracy 0.4575\n",
            "Epoch 4 Batch 850 Loss 1.2014 Accuracy 0.4576\n",
            "Epoch 4 Batch 900 Loss 1.1996 Accuracy 0.4577\n",
            "Epoch 4 Batch 950 Loss 1.1980 Accuracy 0.4577\n",
            "Epoch 4 Batch 1000 Loss 1.1966 Accuracy 0.4576\n",
            "Epoch 4 Batch 1050 Loss 1.1954 Accuracy 0.4578\n",
            "Epoch 4 Batch 1100 Loss 1.1933 Accuracy 0.4578\n",
            "Epoch 4 Batch 1150 Loss 1.1914 Accuracy 0.4580\n",
            "Epoch 4 Batch 1200 Loss 1.1904 Accuracy 0.4583\n",
            "Epoch 4 Batch 1250 Loss 1.1874 Accuracy 0.4587\n",
            "Epoch 4 Batch 1300 Loss 1.1858 Accuracy 0.4588\n",
            "Epoch 4 Batch 1350 Loss 1.1838 Accuracy 0.4593\n",
            "Epoch 4 Batch 1400 Loss 1.1814 Accuracy 0.4601\n",
            "Epoch 4 Batch 1450 Loss 1.1785 Accuracy 0.4608\n",
            "Epoch 4 Batch 1500 Loss 1.1765 Accuracy 0.4614\n",
            "Epoch 4 Batch 1550 Loss 1.1737 Accuracy 0.4622\n",
            "Epoch 4 Batch 1600 Loss 1.1707 Accuracy 0.4632\n",
            "Epoch 4 Batch 1650 Loss 1.1680 Accuracy 0.4640\n",
            "Epoch 4 Batch 1700 Loss 1.1655 Accuracy 0.4647\n",
            "Epoch 4 Batch 1750 Loss 1.1625 Accuracy 0.4656\n",
            "Epoch 4 Batch 1800 Loss 1.1600 Accuracy 0.4663\n",
            "Epoch 4 Batch 1850 Loss 1.1574 Accuracy 0.4671\n",
            "Epoch 4 Batch 1900 Loss 1.1553 Accuracy 0.4679\n",
            "Epoch 4 Batch 1950 Loss 1.1528 Accuracy 0.4686\n",
            "Epoch 4 Batch 2000 Loss 1.1513 Accuracy 0.4694\n",
            "Epoch 4 Batch 2050 Loss 1.1491 Accuracy 0.4698\n",
            "Epoch 4 Batch 2100 Loss 1.1461 Accuracy 0.4702\n",
            "Epoch 4 Batch 2150 Loss 1.1431 Accuracy 0.4704\n",
            "Epoch 4 Batch 2200 Loss 1.1398 Accuracy 0.4707\n",
            "Epoch 4 Batch 2250 Loss 1.1366 Accuracy 0.4709\n",
            "Epoch 4 Batch 2300 Loss 1.1335 Accuracy 0.4710\n",
            "Epoch 4 Batch 2350 Loss 1.1304 Accuracy 0.4713\n",
            "Epoch 4 Batch 2400 Loss 1.1277 Accuracy 0.4716\n",
            "Epoch 4 Batch 2450 Loss 1.1250 Accuracy 0.4719\n",
            "Epoch 4 Batch 2500 Loss 1.1218 Accuracy 0.4723\n",
            "Epoch 4 Batch 2550 Loss 1.1183 Accuracy 0.4727\n",
            "Epoch 4 Batch 2600 Loss 1.1162 Accuracy 0.4730\n",
            "Epoch 4 Batch 2650 Loss 1.1133 Accuracy 0.4733\n",
            "Epoch 4 Batch 2700 Loss 1.1100 Accuracy 0.4737\n",
            "Epoch 4 Batch 2750 Loss 1.1074 Accuracy 0.4740\n",
            "Epoch 4 Batch 2800 Loss 1.1051 Accuracy 0.4744\n",
            "Epoch 4 Batch 2850 Loss 1.1028 Accuracy 0.4746\n",
            "Epoch 4 Batch 2900 Loss 1.1008 Accuracy 0.4750\n",
            "Epoch 4 Batch 2950 Loss 1.0985 Accuracy 0.4753\n",
            "Epoch 4 Batch 3000 Loss 1.0958 Accuracy 0.4756\n",
            "Epoch 4 Batch 3050 Loss 1.0936 Accuracy 0.4759\n",
            "Epoch 4 Batch 3100 Loss 1.0914 Accuracy 0.4762\n",
            "Epoch 4 Batch 3150 Loss 1.0895 Accuracy 0.4764\n",
            "Epoch 4 Batch 3200 Loss 1.0872 Accuracy 0.4767\n",
            "Epoch 4 Batch 3250 Loss 1.0849 Accuracy 0.4769\n",
            "Epoch 4 Batch 3300 Loss 1.0825 Accuracy 0.4772\n",
            "Epoch 4 Batch 3350 Loss 1.0803 Accuracy 0.4775\n",
            "Epoch 4 Batch 3400 Loss 1.0780 Accuracy 0.4779\n",
            "Epoch 4 Batch 3450 Loss 1.0757 Accuracy 0.4782\n",
            "Epoch 4 Batch 3500 Loss 1.0738 Accuracy 0.4785\n",
            "Epoch 4 Batch 3550 Loss 1.0719 Accuracy 0.4788\n",
            "Epoch 4 Batch 3600 Loss 1.0699 Accuracy 0.4792\n",
            "Epoch 4 Batch 3650 Loss 1.0681 Accuracy 0.4795\n",
            "Epoch 4 Batch 3700 Loss 1.0662 Accuracy 0.4799\n",
            "Epoch 4 Batch 3750 Loss 1.0645 Accuracy 0.4803\n",
            "Epoch 4 Batch 3800 Loss 1.0627 Accuracy 0.4807\n",
            "Epoch 4 Batch 3850 Loss 1.0608 Accuracy 0.4811\n",
            "Epoch 4 Batch 3900 Loss 1.0593 Accuracy 0.4814\n",
            "Epoch 4 Batch 3950 Loss 1.0579 Accuracy 0.4818\n",
            "Epoch 4 Batch 4000 Loss 1.0565 Accuracy 0.4821\n",
            "Epoch 4 Batch 4050 Loss 1.0552 Accuracy 0.4825\n",
            "Epoch 4 Batch 4100 Loss 1.0539 Accuracy 0.4828\n",
            "Epoch 4 Batch 4150 Loss 1.0534 Accuracy 0.4829\n",
            "Epoch 4 Batch 4200 Loss 1.0534 Accuracy 0.4829\n",
            "Epoch 4 Batch 4250 Loss 1.0534 Accuracy 0.4829\n",
            "Epoch 4 Batch 4300 Loss 1.0540 Accuracy 0.4829\n",
            "Epoch 4 Batch 4350 Loss 1.0548 Accuracy 0.4828\n",
            "Epoch 4 Batch 4400 Loss 1.0556 Accuracy 0.4827\n",
            "Epoch 4 Batch 4450 Loss 1.0566 Accuracy 0.4826\n",
            "Epoch 4 Batch 4500 Loss 1.0577 Accuracy 0.4824\n",
            "Epoch 4 Batch 4550 Loss 1.0591 Accuracy 0.4822\n",
            "Epoch 4 Batch 4600 Loss 1.0604 Accuracy 0.4820\n",
            "Epoch 4 Batch 4650 Loss 1.0618 Accuracy 0.4819\n",
            "Epoch 4 Batch 4700 Loss 1.0630 Accuracy 0.4818\n",
            "Epoch 4 Batch 4750 Loss 1.0643 Accuracy 0.4816\n",
            "Epoch 4 Batch 4800 Loss 1.0653 Accuracy 0.4815\n",
            "Epoch 4 Batch 4850 Loss 1.0663 Accuracy 0.4814\n",
            "Epoch 4 Batch 4900 Loss 1.0674 Accuracy 0.4812\n",
            "Epoch 4 Batch 4950 Loss 1.0682 Accuracy 0.4810\n",
            "Epoch 4 Batch 5000 Loss 1.0694 Accuracy 0.4808\n",
            "Epoch 4 Batch 5050 Loss 1.0705 Accuracy 0.4807\n",
            "Epoch 4 Batch 5100 Loss 1.0714 Accuracy 0.4804\n",
            "Epoch 4 Batch 5150 Loss 1.0724 Accuracy 0.4802\n",
            "Epoch 4 Batch 5200 Loss 1.0736 Accuracy 0.4799\n",
            "Epoch 4 Batch 5250 Loss 1.0748 Accuracy 0.4797\n",
            "Epoch 4 Batch 5300 Loss 1.0755 Accuracy 0.4795\n",
            "Epoch 4 Batch 5350 Loss 1.0766 Accuracy 0.4792\n",
            "Epoch 4 Batch 5400 Loss 1.0776 Accuracy 0.4790\n",
            "Epoch 4 Batch 5450 Loss 1.0783 Accuracy 0.4788\n",
            "Epoch 4 Batch 5500 Loss 1.0791 Accuracy 0.4786\n",
            "Epoch 4 Batch 5550 Loss 1.0799 Accuracy 0.4783\n",
            "Epoch 4 Batch 5600 Loss 1.0805 Accuracy 0.4781\n",
            "Epoch 4 Batch 5650 Loss 1.0812 Accuracy 0.4779\n",
            "Epoch 4 Batch 5700 Loss 1.0819 Accuracy 0.4777\n",
            "Saving checkpoint for epoch 4 at ./content/drive/My Drive/SelfPractise/DeepNLP/Transformer/ckpt/ckpt-4\n",
            "Time taken for 1 epoch: 1513.993272304535 secs\n",
            "\n",
            "Start of epoch 5\n",
            "Epoch 5 Batch 0 Loss 1.1761 Accuracy 0.4449\n",
            "Epoch 5 Batch 50 Loss 1.1667 Accuracy 0.4638\n",
            "Epoch 5 Batch 100 Loss 1.1550 Accuracy 0.4645\n",
            "Epoch 5 Batch 150 Loss 1.1611 Accuracy 0.4661\n",
            "Epoch 5 Batch 200 Loss 1.1563 Accuracy 0.4664\n",
            "Epoch 5 Batch 250 Loss 1.1555 Accuracy 0.4658\n",
            "Epoch 5 Batch 300 Loss 1.1512 Accuracy 0.4652\n",
            "Epoch 5 Batch 350 Loss 1.1487 Accuracy 0.4654\n",
            "Epoch 5 Batch 400 Loss 1.1488 Accuracy 0.4652\n",
            "Epoch 5 Batch 450 Loss 1.1473 Accuracy 0.4656\n",
            "Epoch 5 Batch 500 Loss 1.1425 Accuracy 0.4658\n",
            "Epoch 5 Batch 550 Loss 1.1420 Accuracy 0.4656\n",
            "Epoch 5 Batch 600 Loss 1.1418 Accuracy 0.4656\n",
            "Epoch 5 Batch 650 Loss 1.1408 Accuracy 0.4659\n",
            "Epoch 5 Batch 700 Loss 1.1416 Accuracy 0.4661\n",
            "Epoch 5 Batch 750 Loss 1.1424 Accuracy 0.4665\n",
            "Epoch 5 Batch 800 Loss 1.1416 Accuracy 0.4665\n",
            "Epoch 5 Batch 850 Loss 1.1412 Accuracy 0.4665\n",
            "Epoch 5 Batch 900 Loss 1.1413 Accuracy 0.4665\n",
            "Epoch 5 Batch 950 Loss 1.1392 Accuracy 0.4664\n",
            "Epoch 5 Batch 1000 Loss 1.1365 Accuracy 0.4665\n",
            "Epoch 5 Batch 1050 Loss 1.1350 Accuracy 0.4667\n",
            "Epoch 5 Batch 1100 Loss 1.1346 Accuracy 0.4668\n",
            "Epoch 5 Batch 1150 Loss 1.1333 Accuracy 0.4669\n",
            "Epoch 5 Batch 1200 Loss 1.1310 Accuracy 0.4671\n",
            "Epoch 5 Batch 1250 Loss 1.1285 Accuracy 0.4676\n",
            "Epoch 5 Batch 1300 Loss 1.1261 Accuracy 0.4680\n",
            "Epoch 5 Batch 1350 Loss 1.1244 Accuracy 0.4685\n",
            "Epoch 5 Batch 1400 Loss 1.1218 Accuracy 0.4693\n",
            "Epoch 5 Batch 1450 Loss 1.1191 Accuracy 0.4700\n",
            "Epoch 5 Batch 1500 Loss 1.1156 Accuracy 0.4709\n",
            "Epoch 5 Batch 1550 Loss 1.1127 Accuracy 0.4718\n",
            "Epoch 5 Batch 1600 Loss 1.1101 Accuracy 0.4727\n",
            "Epoch 5 Batch 1650 Loss 1.1077 Accuracy 0.4733\n",
            "Epoch 5 Batch 1700 Loss 1.1050 Accuracy 0.4741\n",
            "Epoch 5 Batch 1750 Loss 1.1025 Accuracy 0.4749\n",
            "Epoch 5 Batch 1800 Loss 1.0997 Accuracy 0.4756\n",
            "Epoch 5 Batch 1850 Loss 1.0977 Accuracy 0.4765\n",
            "Epoch 5 Batch 1900 Loss 1.0957 Accuracy 0.4772\n",
            "Epoch 5 Batch 1950 Loss 1.0935 Accuracy 0.4779\n",
            "Epoch 5 Batch 2000 Loss 1.0920 Accuracy 0.4786\n",
            "Epoch 5 Batch 2050 Loss 1.0899 Accuracy 0.4791\n",
            "Epoch 5 Batch 2100 Loss 1.0871 Accuracy 0.4795\n",
            "Epoch 5 Batch 2150 Loss 1.0846 Accuracy 0.4798\n",
            "Epoch 5 Batch 2200 Loss 1.0819 Accuracy 0.4800\n",
            "Epoch 5 Batch 2250 Loss 1.0789 Accuracy 0.4802\n",
            "Epoch 5 Batch 2300 Loss 1.0752 Accuracy 0.4803\n",
            "Epoch 5 Batch 2350 Loss 1.0724 Accuracy 0.4807\n",
            "Epoch 5 Batch 2400 Loss 1.0694 Accuracy 0.4810\n",
            "Epoch 5 Batch 2450 Loss 1.0667 Accuracy 0.4812\n",
            "Epoch 5 Batch 2500 Loss 1.0638 Accuracy 0.4814\n",
            "Epoch 5 Batch 2550 Loss 1.0611 Accuracy 0.4818\n",
            "Epoch 5 Batch 2600 Loss 1.0586 Accuracy 0.4821\n",
            "Epoch 5 Batch 2650 Loss 1.0560 Accuracy 0.4825\n",
            "Epoch 5 Batch 2700 Loss 1.0533 Accuracy 0.4829\n",
            "Epoch 5 Batch 2750 Loss 1.0506 Accuracy 0.4832\n",
            "Epoch 5 Batch 2800 Loss 1.0480 Accuracy 0.4835\n",
            "Epoch 5 Batch 2850 Loss 1.0458 Accuracy 0.4838\n",
            "Epoch 5 Batch 2900 Loss 1.0439 Accuracy 0.4842\n",
            "Epoch 5 Batch 2950 Loss 1.0415 Accuracy 0.4845\n",
            "Epoch 5 Batch 3000 Loss 1.0394 Accuracy 0.4848\n",
            "Epoch 5 Batch 3050 Loss 1.0376 Accuracy 0.4851\n",
            "Epoch 5 Batch 3100 Loss 1.0356 Accuracy 0.4853\n",
            "Epoch 5 Batch 3150 Loss 1.0334 Accuracy 0.4856\n",
            "Epoch 5 Batch 3200 Loss 1.0312 Accuracy 0.4858\n",
            "Epoch 5 Batch 3250 Loss 1.0293 Accuracy 0.4860\n",
            "Epoch 5 Batch 3300 Loss 1.0272 Accuracy 0.4863\n",
            "Epoch 5 Batch 3350 Loss 1.0246 Accuracy 0.4866\n",
            "Epoch 5 Batch 3400 Loss 1.0227 Accuracy 0.4870\n",
            "Epoch 5 Batch 3450 Loss 1.0210 Accuracy 0.4872\n",
            "Epoch 5 Batch 3500 Loss 1.0189 Accuracy 0.4876\n",
            "Epoch 5 Batch 3550 Loss 1.0169 Accuracy 0.4880\n",
            "Epoch 5 Batch 3600 Loss 1.0150 Accuracy 0.4882\n",
            "Epoch 5 Batch 3650 Loss 1.0130 Accuracy 0.4886\n",
            "Epoch 5 Batch 3700 Loss 1.0115 Accuracy 0.4890\n",
            "Epoch 5 Batch 3750 Loss 1.0097 Accuracy 0.4893\n",
            "Epoch 5 Batch 3800 Loss 1.0083 Accuracy 0.4897\n",
            "Epoch 5 Batch 3850 Loss 1.0069 Accuracy 0.4901\n",
            "Epoch 5 Batch 3900 Loss 1.0054 Accuracy 0.4904\n",
            "Epoch 5 Batch 3950 Loss 1.0040 Accuracy 0.4907\n",
            "Epoch 5 Batch 4000 Loss 1.0023 Accuracy 0.4911\n",
            "Epoch 5 Batch 4050 Loss 1.0008 Accuracy 0.4913\n",
            "Epoch 5 Batch 4100 Loss 0.9999 Accuracy 0.4916\n",
            "Epoch 5 Batch 4150 Loss 0.9992 Accuracy 0.4918\n",
            "Epoch 5 Batch 4200 Loss 0.9992 Accuracy 0.4918\n",
            "Epoch 5 Batch 4250 Loss 0.9995 Accuracy 0.4918\n",
            "Epoch 5 Batch 4300 Loss 1.0005 Accuracy 0.4916\n",
            "Epoch 5 Batch 4350 Loss 1.0015 Accuracy 0.4915\n",
            "Epoch 5 Batch 4400 Loss 1.0025 Accuracy 0.4914\n",
            "Epoch 5 Batch 4450 Loss 1.0034 Accuracy 0.4912\n",
            "Epoch 5 Batch 4500 Loss 1.0043 Accuracy 0.4911\n",
            "Epoch 5 Batch 4550 Loss 1.0057 Accuracy 0.4908\n",
            "Epoch 5 Batch 4600 Loss 1.0069 Accuracy 0.4906\n",
            "Epoch 5 Batch 4650 Loss 1.0085 Accuracy 0.4905\n",
            "Epoch 5 Batch 4700 Loss 1.0099 Accuracy 0.4903\n",
            "Epoch 5 Batch 4750 Loss 1.0111 Accuracy 0.4901\n",
            "Epoch 5 Batch 4800 Loss 1.0119 Accuracy 0.4899\n",
            "Epoch 5 Batch 4850 Loss 1.0129 Accuracy 0.4898\n",
            "Epoch 5 Batch 4900 Loss 1.0142 Accuracy 0.4897\n",
            "Epoch 5 Batch 4950 Loss 1.0155 Accuracy 0.4895\n",
            "Epoch 5 Batch 5000 Loss 1.0169 Accuracy 0.4893\n",
            "Epoch 5 Batch 5050 Loss 1.0180 Accuracy 0.4891\n",
            "Epoch 5 Batch 5100 Loss 1.0191 Accuracy 0.4889\n",
            "Epoch 5 Batch 5150 Loss 1.0205 Accuracy 0.4887\n",
            "Epoch 5 Batch 5200 Loss 1.0215 Accuracy 0.4885\n",
            "Epoch 5 Batch 5250 Loss 1.0226 Accuracy 0.4882\n",
            "Epoch 5 Batch 5300 Loss 1.0237 Accuracy 0.4879\n",
            "Epoch 5 Batch 5350 Loss 1.0248 Accuracy 0.4876\n",
            "Epoch 5 Batch 5400 Loss 1.0258 Accuracy 0.4874\n",
            "Epoch 5 Batch 5450 Loss 1.0265 Accuracy 0.4872\n",
            "Epoch 5 Batch 5500 Loss 1.0274 Accuracy 0.4870\n",
            "Epoch 5 Batch 5550 Loss 1.0282 Accuracy 0.4868\n",
            "Epoch 5 Batch 5600 Loss 1.0291 Accuracy 0.4865\n",
            "Epoch 5 Batch 5650 Loss 1.0299 Accuracy 0.4863\n",
            "Epoch 5 Batch 5700 Loss 1.0308 Accuracy 0.4860\n",
            "Saving checkpoint for epoch 5 at ./content/drive/My Drive/SelfPractise/DeepNLP/Transformer/ckpt/ckpt-5\n",
            "Time taken for 1 epoch: 1506.755210161209 secs\n",
            "\n",
            "Start of epoch 6\n",
            "Epoch 6 Batch 0 Loss 1.1415 Accuracy 0.4778\n",
            "Epoch 6 Batch 50 Loss 1.1056 Accuracy 0.4725\n",
            "Epoch 6 Batch 100 Loss 1.1057 Accuracy 0.4730\n",
            "Epoch 6 Batch 150 Loss 1.1005 Accuracy 0.4711\n",
            "Epoch 6 Batch 200 Loss 1.1083 Accuracy 0.4711\n",
            "Epoch 6 Batch 250 Loss 1.1154 Accuracy 0.4710\n",
            "Epoch 6 Batch 300 Loss 1.1138 Accuracy 0.4705\n",
            "Epoch 6 Batch 350 Loss 1.1070 Accuracy 0.4713\n",
            "Epoch 6 Batch 400 Loss 1.1053 Accuracy 0.4717\n",
            "Epoch 6 Batch 450 Loss 1.1048 Accuracy 0.4716\n",
            "Epoch 6 Batch 500 Loss 1.1026 Accuracy 0.4718\n",
            "Epoch 6 Batch 550 Loss 1.1007 Accuracy 0.4720\n",
            "Epoch 6 Batch 600 Loss 1.1020 Accuracy 0.4727\n",
            "Epoch 6 Batch 650 Loss 1.1016 Accuracy 0.4728\n",
            "Epoch 6 Batch 700 Loss 1.1014 Accuracy 0.4724\n",
            "Epoch 6 Batch 750 Loss 1.1004 Accuracy 0.4728\n",
            "Epoch 6 Batch 800 Loss 1.0996 Accuracy 0.4732\n",
            "Epoch 6 Batch 850 Loss 1.1003 Accuracy 0.4732\n",
            "Epoch 6 Batch 900 Loss 1.0988 Accuracy 0.4734\n",
            "Epoch 6 Batch 950 Loss 1.0980 Accuracy 0.4731\n",
            "Epoch 6 Batch 1000 Loss 1.0967 Accuracy 0.4731\n",
            "Epoch 6 Batch 1050 Loss 1.0957 Accuracy 0.4734\n",
            "Epoch 6 Batch 1100 Loss 1.0945 Accuracy 0.4735\n",
            "Epoch 6 Batch 1150 Loss 1.0925 Accuracy 0.4737\n",
            "Epoch 6 Batch 1200 Loss 1.0899 Accuracy 0.4740\n",
            "Epoch 6 Batch 1250 Loss 1.0874 Accuracy 0.4745\n",
            "Epoch 6 Batch 1300 Loss 1.0855 Accuracy 0.4750\n",
            "Epoch 6 Batch 1350 Loss 1.0831 Accuracy 0.4754\n",
            "Epoch 6 Batch 1400 Loss 1.0806 Accuracy 0.4760\n",
            "Epoch 6 Batch 1450 Loss 1.0786 Accuracy 0.4767\n",
            "Epoch 6 Batch 1500 Loss 1.0758 Accuracy 0.4775\n",
            "Epoch 6 Batch 1550 Loss 1.0732 Accuracy 0.4783\n",
            "Epoch 6 Batch 1600 Loss 1.0705 Accuracy 0.4792\n",
            "Epoch 6 Batch 1650 Loss 1.0686 Accuracy 0.4801\n",
            "Epoch 6 Batch 1700 Loss 1.0669 Accuracy 0.4808\n",
            "Epoch 6 Batch 1750 Loss 1.0643 Accuracy 0.4818\n",
            "Epoch 6 Batch 1800 Loss 1.0614 Accuracy 0.4825\n",
            "Epoch 6 Batch 1850 Loss 1.0594 Accuracy 0.4833\n",
            "Epoch 6 Batch 1900 Loss 1.0572 Accuracy 0.4840\n",
            "Epoch 6 Batch 1950 Loss 1.0547 Accuracy 0.4846\n",
            "Epoch 6 Batch 2000 Loss 1.0528 Accuracy 0.4853\n",
            "Epoch 6 Batch 2050 Loss 1.0509 Accuracy 0.4859\n",
            "Epoch 6 Batch 2100 Loss 1.0490 Accuracy 0.4862\n",
            "Epoch 6 Batch 2150 Loss 1.0462 Accuracy 0.4865\n",
            "Epoch 6 Batch 2200 Loss 1.0431 Accuracy 0.4866\n",
            "Epoch 6 Batch 2250 Loss 1.0402 Accuracy 0.4868\n",
            "Epoch 6 Batch 2300 Loss 1.0372 Accuracy 0.4870\n",
            "Epoch 6 Batch 2350 Loss 1.0343 Accuracy 0.4873\n",
            "Epoch 6 Batch 2400 Loss 1.0312 Accuracy 0.4874\n",
            "Epoch 6 Batch 2450 Loss 1.0280 Accuracy 0.4877\n",
            "Epoch 6 Batch 2500 Loss 1.0252 Accuracy 0.4880\n",
            "Epoch 6 Batch 2550 Loss 1.0224 Accuracy 0.4883\n",
            "Epoch 6 Batch 2600 Loss 1.0196 Accuracy 0.4887\n",
            "Epoch 6 Batch 2650 Loss 1.0169 Accuracy 0.4890\n",
            "Epoch 6 Batch 2700 Loss 1.0145 Accuracy 0.4894\n",
            "Epoch 6 Batch 2750 Loss 1.0120 Accuracy 0.4896\n",
            "Epoch 6 Batch 2800 Loss 1.0099 Accuracy 0.4898\n",
            "Epoch 6 Batch 2850 Loss 1.0077 Accuracy 0.4902\n",
            "Epoch 6 Batch 2900 Loss 1.0056 Accuracy 0.4905\n",
            "Epoch 6 Batch 2950 Loss 1.0034 Accuracy 0.4908\n",
            "Epoch 6 Batch 3000 Loss 1.0009 Accuracy 0.4911\n",
            "Epoch 6 Batch 3050 Loss 0.9991 Accuracy 0.4913\n",
            "Epoch 6 Batch 3100 Loss 0.9975 Accuracy 0.4916\n",
            "Epoch 6 Batch 3150 Loss 0.9957 Accuracy 0.4918\n",
            "Epoch 6 Batch 3200 Loss 0.9936 Accuracy 0.4920\n",
            "Epoch 6 Batch 3250 Loss 0.9915 Accuracy 0.4923\n",
            "Epoch 6 Batch 3300 Loss 0.9896 Accuracy 0.4926\n",
            "Epoch 6 Batch 3350 Loss 0.9876 Accuracy 0.4929\n",
            "Epoch 6 Batch 3400 Loss 0.9856 Accuracy 0.4931\n",
            "Epoch 6 Batch 3450 Loss 0.9839 Accuracy 0.4934\n",
            "Epoch 6 Batch 3500 Loss 0.9820 Accuracy 0.4937\n",
            "Epoch 6 Batch 3550 Loss 0.9804 Accuracy 0.4940\n",
            "Epoch 6 Batch 3600 Loss 0.9782 Accuracy 0.4943\n",
            "Epoch 6 Batch 3650 Loss 0.9764 Accuracy 0.4945\n",
            "Epoch 6 Batch 3700 Loss 0.9745 Accuracy 0.4949\n",
            "Epoch 6 Batch 3750 Loss 0.9729 Accuracy 0.4953\n",
            "Epoch 6 Batch 3800 Loss 0.9713 Accuracy 0.4956\n",
            "Epoch 6 Batch 3850 Loss 0.9698 Accuracy 0.4960\n",
            "Epoch 6 Batch 3900 Loss 0.9686 Accuracy 0.4964\n",
            "Epoch 6 Batch 3950 Loss 0.9675 Accuracy 0.4967\n",
            "Epoch 6 Batch 4000 Loss 0.9662 Accuracy 0.4971\n",
            "Epoch 6 Batch 4050 Loss 0.9647 Accuracy 0.4974\n",
            "Epoch 6 Batch 4100 Loss 0.9639 Accuracy 0.4976\n",
            "Epoch 6 Batch 4150 Loss 0.9636 Accuracy 0.4976\n",
            "Epoch 6 Batch 4200 Loss 0.9634 Accuracy 0.4977\n",
            "Epoch 6 Batch 4250 Loss 0.9639 Accuracy 0.4976\n",
            "Epoch 6 Batch 4300 Loss 0.9643 Accuracy 0.4975\n",
            "Epoch 6 Batch 4350 Loss 0.9656 Accuracy 0.4974\n",
            "Epoch 6 Batch 4400 Loss 0.9666 Accuracy 0.4972\n",
            "Epoch 6 Batch 4450 Loss 0.9676 Accuracy 0.4971\n",
            "Epoch 6 Batch 4500 Loss 0.9685 Accuracy 0.4969\n",
            "Epoch 6 Batch 4550 Loss 0.9700 Accuracy 0.4967\n",
            "Epoch 6 Batch 4600 Loss 0.9714 Accuracy 0.4965\n",
            "Epoch 6 Batch 4650 Loss 0.9727 Accuracy 0.4964\n",
            "Epoch 6 Batch 4700 Loss 0.9738 Accuracy 0.4962\n",
            "Epoch 6 Batch 4750 Loss 0.9751 Accuracy 0.4960\n",
            "Epoch 6 Batch 4800 Loss 0.9763 Accuracy 0.4958\n",
            "Epoch 6 Batch 4850 Loss 0.9774 Accuracy 0.4957\n",
            "Epoch 6 Batch 4900 Loss 0.9786 Accuracy 0.4955\n",
            "Epoch 6 Batch 4950 Loss 0.9801 Accuracy 0.4953\n",
            "Epoch 6 Batch 5000 Loss 0.9815 Accuracy 0.4951\n",
            "Epoch 6 Batch 5050 Loss 0.9826 Accuracy 0.4949\n",
            "Epoch 6 Batch 5100 Loss 0.9838 Accuracy 0.4947\n",
            "Epoch 6 Batch 5150 Loss 0.9848 Accuracy 0.4944\n",
            "Epoch 6 Batch 5200 Loss 0.9858 Accuracy 0.4942\n",
            "Epoch 6 Batch 5250 Loss 0.9870 Accuracy 0.4939\n",
            "Epoch 6 Batch 5300 Loss 0.9880 Accuracy 0.4936\n",
            "Epoch 6 Batch 5350 Loss 0.9892 Accuracy 0.4934\n",
            "Epoch 6 Batch 5400 Loss 0.9901 Accuracy 0.4932\n",
            "Epoch 6 Batch 5450 Loss 0.9913 Accuracy 0.4929\n",
            "Epoch 6 Batch 5500 Loss 0.9923 Accuracy 0.4927\n",
            "Epoch 6 Batch 5550 Loss 0.9932 Accuracy 0.4924\n",
            "Epoch 6 Batch 5600 Loss 0.9943 Accuracy 0.4922\n",
            "Epoch 6 Batch 5650 Loss 0.9951 Accuracy 0.4919\n",
            "Epoch 6 Batch 5700 Loss 0.9959 Accuracy 0.4917\n",
            "Saving checkpoint for epoch 6 at ./content/drive/My Drive/SelfPractise/DeepNLP/Transformer/ckpt/ckpt-6\n",
            "Time taken for 1 epoch: 1488.6122574806213 secs\n",
            "\n",
            "Start of epoch 7\n",
            "Epoch 7 Batch 0 Loss 0.9876 Accuracy 0.4901\n",
            "Epoch 7 Batch 50 Loss 1.1095 Accuracy 0.4745\n",
            "Epoch 7 Batch 100 Loss 1.0888 Accuracy 0.4752\n",
            "Epoch 7 Batch 150 Loss 1.0980 Accuracy 0.4768\n",
            "Epoch 7 Batch 200 Loss 1.0932 Accuracy 0.4772\n",
            "Epoch 7 Batch 250 Loss 1.0928 Accuracy 0.4771\n",
            "Epoch 7 Batch 300 Loss 1.0901 Accuracy 0.4761\n",
            "Epoch 7 Batch 350 Loss 1.0857 Accuracy 0.4770\n",
            "Epoch 7 Batch 400 Loss 1.0832 Accuracy 0.4763\n",
            "Epoch 7 Batch 450 Loss 1.0793 Accuracy 0.4767\n",
            "Epoch 7 Batch 500 Loss 1.0778 Accuracy 0.4767\n",
            "Epoch 7 Batch 550 Loss 1.0776 Accuracy 0.4766\n",
            "Epoch 7 Batch 600 Loss 1.0738 Accuracy 0.4770\n",
            "Epoch 7 Batch 650 Loss 1.0731 Accuracy 0.4774\n",
            "Epoch 7 Batch 700 Loss 1.0722 Accuracy 0.4776\n",
            "Epoch 7 Batch 750 Loss 1.0719 Accuracy 0.4780\n",
            "Epoch 7 Batch 800 Loss 1.0703 Accuracy 0.4784\n",
            "Epoch 7 Batch 850 Loss 1.0684 Accuracy 0.4788\n",
            "Epoch 7 Batch 900 Loss 1.0690 Accuracy 0.4789\n",
            "Epoch 7 Batch 950 Loss 1.0668 Accuracy 0.4788\n",
            "Epoch 7 Batch 1000 Loss 1.0645 Accuracy 0.4786\n",
            "Epoch 7 Batch 1050 Loss 1.0633 Accuracy 0.4785\n",
            "Epoch 7 Batch 1100 Loss 1.0623 Accuracy 0.4785\n",
            "Epoch 7 Batch 1150 Loss 1.0617 Accuracy 0.4786\n",
            "Epoch 7 Batch 1200 Loss 1.0603 Accuracy 0.4787\n",
            "Epoch 7 Batch 1250 Loss 1.0582 Accuracy 0.4790\n",
            "Epoch 7 Batch 1300 Loss 1.0570 Accuracy 0.4794\n",
            "Epoch 7 Batch 1350 Loss 1.0546 Accuracy 0.4801\n",
            "Epoch 7 Batch 1400 Loss 1.0525 Accuracy 0.4808\n",
            "Epoch 7 Batch 1450 Loss 1.0507 Accuracy 0.4815\n",
            "Epoch 7 Batch 1500 Loss 1.0480 Accuracy 0.4823\n",
            "Epoch 7 Batch 1550 Loss 1.0450 Accuracy 0.4831\n",
            "Epoch 7 Batch 1600 Loss 1.0429 Accuracy 0.4839\n",
            "Epoch 7 Batch 1650 Loss 1.0406 Accuracy 0.4849\n",
            "Epoch 7 Batch 1700 Loss 1.0390 Accuracy 0.4856\n",
            "Epoch 7 Batch 1750 Loss 1.0357 Accuracy 0.4865\n",
            "Epoch 7 Batch 1800 Loss 1.0330 Accuracy 0.4872\n",
            "Epoch 7 Batch 1850 Loss 1.0305 Accuracy 0.4879\n",
            "Epoch 7 Batch 1900 Loss 1.0285 Accuracy 0.4887\n",
            "Epoch 7 Batch 1950 Loss 1.0265 Accuracy 0.4895\n",
            "Epoch 7 Batch 2000 Loss 1.0245 Accuracy 0.4901\n",
            "Epoch 7 Batch 2050 Loss 1.0222 Accuracy 0.4906\n",
            "Epoch 7 Batch 2100 Loss 1.0201 Accuracy 0.4909\n",
            "Epoch 7 Batch 2150 Loss 1.0174 Accuracy 0.4911\n",
            "Epoch 7 Batch 2200 Loss 1.0141 Accuracy 0.4913\n",
            "Epoch 7 Batch 2250 Loss 1.0111 Accuracy 0.4916\n",
            "Epoch 7 Batch 2300 Loss 1.0083 Accuracy 0.4918\n",
            "Epoch 7 Batch 2350 Loss 1.0051 Accuracy 0.4921\n",
            "Epoch 7 Batch 2400 Loss 1.0023 Accuracy 0.4923\n",
            "Epoch 7 Batch 2450 Loss 0.9995 Accuracy 0.4926\n",
            "Epoch 7 Batch 2500 Loss 0.9967 Accuracy 0.4928\n",
            "Epoch 7 Batch 2550 Loss 0.9940 Accuracy 0.4932\n",
            "Epoch 7 Batch 2600 Loss 0.9917 Accuracy 0.4935\n",
            "Epoch 7 Batch 2650 Loss 0.9888 Accuracy 0.4939\n",
            "Epoch 7 Batch 2700 Loss 0.9859 Accuracy 0.4942\n",
            "Epoch 7 Batch 2750 Loss 0.9830 Accuracy 0.4944\n",
            "Epoch 7 Batch 2800 Loss 0.9807 Accuracy 0.4948\n",
            "Epoch 7 Batch 2850 Loss 0.9785 Accuracy 0.4951\n",
            "Epoch 7 Batch 2900 Loss 0.9767 Accuracy 0.4953\n",
            "Epoch 7 Batch 2950 Loss 0.9746 Accuracy 0.4957\n",
            "Epoch 7 Batch 3000 Loss 0.9727 Accuracy 0.4960\n",
            "Epoch 7 Batch 3050 Loss 0.9711 Accuracy 0.4962\n",
            "Epoch 7 Batch 3100 Loss 0.9691 Accuracy 0.4965\n",
            "Epoch 7 Batch 3150 Loss 0.9673 Accuracy 0.4967\n",
            "Epoch 7 Batch 3200 Loss 0.9654 Accuracy 0.4969\n",
            "Epoch 7 Batch 3250 Loss 0.9635 Accuracy 0.4971\n",
            "Epoch 7 Batch 3300 Loss 0.9615 Accuracy 0.4973\n",
            "Epoch 7 Batch 3350 Loss 0.9594 Accuracy 0.4976\n",
            "Epoch 7 Batch 3400 Loss 0.9575 Accuracy 0.4978\n",
            "Epoch 7 Batch 3450 Loss 0.9557 Accuracy 0.4982\n",
            "Epoch 7 Batch 3500 Loss 0.9542 Accuracy 0.4984\n",
            "Epoch 7 Batch 3550 Loss 0.9523 Accuracy 0.4987\n",
            "Epoch 7 Batch 3600 Loss 0.9504 Accuracy 0.4991\n",
            "Epoch 7 Batch 3650 Loss 0.9486 Accuracy 0.4994\n",
            "Epoch 7 Batch 3700 Loss 0.9472 Accuracy 0.4997\n",
            "Epoch 7 Batch 3750 Loss 0.9456 Accuracy 0.5001\n",
            "Epoch 7 Batch 3800 Loss 0.9442 Accuracy 0.5004\n",
            "Epoch 7 Batch 3850 Loss 0.9428 Accuracy 0.5007\n",
            "Epoch 7 Batch 3900 Loss 0.9412 Accuracy 0.5010\n",
            "Epoch 7 Batch 3950 Loss 0.9398 Accuracy 0.5014\n",
            "Epoch 7 Batch 4000 Loss 0.9384 Accuracy 0.5017\n",
            "Epoch 7 Batch 4050 Loss 0.9371 Accuracy 0.5020\n",
            "Epoch 7 Batch 4100 Loss 0.9359 Accuracy 0.5022\n",
            "Epoch 7 Batch 4150 Loss 0.9356 Accuracy 0.5022\n",
            "Epoch 7 Batch 4200 Loss 0.9356 Accuracy 0.5023\n",
            "Epoch 7 Batch 4250 Loss 0.9359 Accuracy 0.5023\n",
            "Epoch 7 Batch 4300 Loss 0.9369 Accuracy 0.5023\n",
            "Epoch 7 Batch 4350 Loss 0.9377 Accuracy 0.5022\n",
            "Epoch 7 Batch 4400 Loss 0.9388 Accuracy 0.5020\n",
            "Epoch 7 Batch 4450 Loss 0.9400 Accuracy 0.5018\n",
            "Epoch 7 Batch 4500 Loss 0.9413 Accuracy 0.5016\n",
            "Epoch 7 Batch 4550 Loss 0.9428 Accuracy 0.5014\n",
            "Epoch 7 Batch 4600 Loss 0.9440 Accuracy 0.5012\n",
            "Epoch 7 Batch 4650 Loss 0.9456 Accuracy 0.5011\n",
            "Epoch 7 Batch 4700 Loss 0.9471 Accuracy 0.5009\n",
            "Epoch 7 Batch 4750 Loss 0.9485 Accuracy 0.5007\n",
            "Epoch 7 Batch 4800 Loss 0.9495 Accuracy 0.5005\n",
            "Epoch 7 Batch 4850 Loss 0.9504 Accuracy 0.5003\n",
            "Epoch 7 Batch 4900 Loss 0.9515 Accuracy 0.5001\n",
            "Epoch 7 Batch 4950 Loss 0.9528 Accuracy 0.4999\n",
            "Epoch 7 Batch 5000 Loss 0.9540 Accuracy 0.4997\n",
            "Epoch 7 Batch 5050 Loss 0.9554 Accuracy 0.4995\n",
            "Epoch 7 Batch 5100 Loss 0.9566 Accuracy 0.4993\n",
            "Epoch 7 Batch 5150 Loss 0.9579 Accuracy 0.4990\n",
            "Epoch 7 Batch 5200 Loss 0.9592 Accuracy 0.4987\n",
            "Epoch 7 Batch 5250 Loss 0.9606 Accuracy 0.4985\n",
            "Epoch 7 Batch 5300 Loss 0.9616 Accuracy 0.4982\n",
            "Epoch 7 Batch 5350 Loss 0.9630 Accuracy 0.4979\n",
            "Epoch 7 Batch 5400 Loss 0.9639 Accuracy 0.4977\n",
            "Epoch 7 Batch 5450 Loss 0.9649 Accuracy 0.4974\n",
            "Epoch 7 Batch 5500 Loss 0.9659 Accuracy 0.4971\n",
            "Epoch 7 Batch 5550 Loss 0.9669 Accuracy 0.4969\n",
            "Epoch 7 Batch 5600 Loss 0.9676 Accuracy 0.4966\n",
            "Epoch 7 Batch 5650 Loss 0.9684 Accuracy 0.4964\n",
            "Epoch 7 Batch 5700 Loss 0.9693 Accuracy 0.4962\n",
            "Saving checkpoint for epoch 7 at ./content/drive/My Drive/SelfPractise/DeepNLP/Transformer/ckpt/ckpt-7\n",
            "Time taken for 1 epoch: 1493.3049488067627 secs\n",
            "\n",
            "Start of epoch 8\n",
            "Epoch 8 Batch 0 Loss 1.0925 Accuracy 0.4556\n",
            "Epoch 8 Batch 50 Loss 1.0861 Accuracy 0.4790\n",
            "Epoch 8 Batch 100 Loss 1.0677 Accuracy 0.4777\n",
            "Epoch 8 Batch 150 Loss 1.0677 Accuracy 0.4787\n",
            "Epoch 8 Batch 200 Loss 1.0691 Accuracy 0.4787\n",
            "Epoch 8 Batch 250 Loss 1.0684 Accuracy 0.4799\n",
            "Epoch 8 Batch 300 Loss 1.0606 Accuracy 0.4804\n",
            "Epoch 8 Batch 350 Loss 1.0598 Accuracy 0.4809\n",
            "Epoch 8 Batch 400 Loss 1.0568 Accuracy 0.4814\n",
            "Epoch 8 Batch 450 Loss 1.0543 Accuracy 0.4813\n",
            "Epoch 8 Batch 500 Loss 1.0506 Accuracy 0.4812\n",
            "Epoch 8 Batch 550 Loss 1.0494 Accuracy 0.4808\n",
            "Epoch 8 Batch 600 Loss 1.0492 Accuracy 0.4807\n",
            "Epoch 8 Batch 650 Loss 1.0503 Accuracy 0.4809\n",
            "Epoch 8 Batch 700 Loss 1.0490 Accuracy 0.4809\n",
            "Epoch 8 Batch 750 Loss 1.0471 Accuracy 0.4813\n",
            "Epoch 8 Batch 800 Loss 1.0456 Accuracy 0.4815\n",
            "Epoch 8 Batch 850 Loss 1.0445 Accuracy 0.4817\n",
            "Epoch 8 Batch 900 Loss 1.0439 Accuracy 0.4819\n",
            "Epoch 8 Batch 950 Loss 1.0427 Accuracy 0.4819\n",
            "Epoch 8 Batch 1000 Loss 1.0420 Accuracy 0.4819\n",
            "Epoch 8 Batch 1050 Loss 1.0405 Accuracy 0.4821\n",
            "Epoch 8 Batch 1100 Loss 1.0389 Accuracy 0.4822\n",
            "Epoch 8 Batch 1150 Loss 1.0380 Accuracy 0.4824\n",
            "Epoch 8 Batch 1200 Loss 1.0364 Accuracy 0.4827\n",
            "Epoch 8 Batch 1250 Loss 1.0344 Accuracy 0.4832\n",
            "Epoch 8 Batch 1300 Loss 1.0326 Accuracy 0.4836\n",
            "Epoch 8 Batch 1350 Loss 1.0303 Accuracy 0.4839\n",
            "Epoch 8 Batch 1400 Loss 1.0283 Accuracy 0.4844\n",
            "Epoch 8 Batch 1450 Loss 1.0263 Accuracy 0.4852\n",
            "Epoch 8 Batch 1500 Loss 1.0238 Accuracy 0.4860\n",
            "Epoch 8 Batch 1550 Loss 1.0211 Accuracy 0.4869\n",
            "Epoch 8 Batch 1600 Loss 1.0188 Accuracy 0.4878\n",
            "Epoch 8 Batch 1650 Loss 1.0165 Accuracy 0.4884\n",
            "Epoch 8 Batch 1700 Loss 1.0143 Accuracy 0.4894\n",
            "Epoch 8 Batch 1750 Loss 1.0121 Accuracy 0.4903\n",
            "Epoch 8 Batch 1800 Loss 1.0095 Accuracy 0.4911\n",
            "Epoch 8 Batch 1850 Loss 1.0076 Accuracy 0.4919\n",
            "Epoch 8 Batch 1900 Loss 1.0051 Accuracy 0.4927\n",
            "Epoch 8 Batch 1950 Loss 1.0031 Accuracy 0.4935\n",
            "Epoch 8 Batch 2000 Loss 1.0007 Accuracy 0.4940\n",
            "Epoch 8 Batch 2050 Loss 0.9989 Accuracy 0.4943\n",
            "Epoch 8 Batch 2100 Loss 0.9965 Accuracy 0.4946\n",
            "Epoch 8 Batch 2150 Loss 0.9939 Accuracy 0.4949\n",
            "Epoch 8 Batch 2200 Loss 0.9907 Accuracy 0.4952\n",
            "Epoch 8 Batch 2250 Loss 0.9875 Accuracy 0.4953\n",
            "Epoch 8 Batch 2300 Loss 0.9845 Accuracy 0.4956\n",
            "Epoch 8 Batch 2350 Loss 0.9813 Accuracy 0.4957\n",
            "Epoch 8 Batch 2400 Loss 0.9784 Accuracy 0.4960\n",
            "Epoch 8 Batch 2450 Loss 0.9757 Accuracy 0.4962\n",
            "Epoch 8 Batch 2500 Loss 0.9731 Accuracy 0.4965\n",
            "Epoch 8 Batch 2550 Loss 0.9702 Accuracy 0.4967\n",
            "Epoch 8 Batch 2600 Loss 0.9676 Accuracy 0.4970\n",
            "Epoch 8 Batch 2650 Loss 0.9651 Accuracy 0.4975\n",
            "Epoch 8 Batch 2700 Loss 0.9624 Accuracy 0.4979\n",
            "Epoch 8 Batch 2750 Loss 0.9598 Accuracy 0.4983\n",
            "Epoch 8 Batch 2800 Loss 0.9580 Accuracy 0.4985\n",
            "Epoch 8 Batch 2850 Loss 0.9561 Accuracy 0.4989\n",
            "Epoch 8 Batch 2900 Loss 0.9538 Accuracy 0.4990\n",
            "Epoch 8 Batch 2950 Loss 0.9517 Accuracy 0.4993\n",
            "Epoch 8 Batch 3000 Loss 0.9501 Accuracy 0.4996\n",
            "Epoch 8 Batch 3050 Loss 0.9481 Accuracy 0.4999\n",
            "Epoch 8 Batch 3100 Loss 0.9460 Accuracy 0.5001\n",
            "Epoch 8 Batch 3150 Loss 0.9441 Accuracy 0.5003\n",
            "Epoch 8 Batch 3200 Loss 0.9421 Accuracy 0.5005\n",
            "Epoch 8 Batch 3250 Loss 0.9402 Accuracy 0.5007\n",
            "Epoch 8 Batch 3300 Loss 0.9383 Accuracy 0.5009\n",
            "Epoch 8 Batch 3350 Loss 0.9363 Accuracy 0.5012\n",
            "Epoch 8 Batch 3400 Loss 0.9344 Accuracy 0.5015\n",
            "Epoch 8 Batch 3450 Loss 0.9327 Accuracy 0.5018\n",
            "Epoch 8 Batch 3500 Loss 0.9314 Accuracy 0.5022\n",
            "Epoch 8 Batch 3550 Loss 0.9300 Accuracy 0.5024\n",
            "Epoch 8 Batch 3600 Loss 0.9282 Accuracy 0.5028\n",
            "Epoch 8 Batch 3650 Loss 0.9263 Accuracy 0.5031\n",
            "Epoch 8 Batch 3700 Loss 0.9247 Accuracy 0.5034\n",
            "Epoch 8 Batch 3750 Loss 0.9233 Accuracy 0.5037\n",
            "Epoch 8 Batch 3800 Loss 0.9220 Accuracy 0.5040\n",
            "Epoch 8 Batch 3850 Loss 0.9205 Accuracy 0.5044\n",
            "Epoch 8 Batch 3900 Loss 0.9193 Accuracy 0.5047\n",
            "Epoch 8 Batch 3950 Loss 0.9178 Accuracy 0.5050\n",
            "Epoch 8 Batch 4000 Loss 0.9164 Accuracy 0.5053\n",
            "Epoch 8 Batch 4050 Loss 0.9152 Accuracy 0.5056\n",
            "Epoch 8 Batch 4100 Loss 0.9142 Accuracy 0.5058\n",
            "Epoch 8 Batch 4150 Loss 0.9138 Accuracy 0.5059\n",
            "Epoch 8 Batch 4200 Loss 0.9139 Accuracy 0.5059\n",
            "Epoch 8 Batch 4250 Loss 0.9141 Accuracy 0.5059\n",
            "Epoch 8 Batch 4300 Loss 0.9145 Accuracy 0.5059\n",
            "Epoch 8 Batch 4350 Loss 0.9155 Accuracy 0.5057\n",
            "Epoch 8 Batch 4400 Loss 0.9170 Accuracy 0.5055\n",
            "Epoch 8 Batch 4450 Loss 0.9183 Accuracy 0.5054\n",
            "Epoch 8 Batch 4500 Loss 0.9198 Accuracy 0.5051\n",
            "Epoch 8 Batch 4550 Loss 0.9212 Accuracy 0.5050\n",
            "Epoch 8 Batch 4600 Loss 0.9224 Accuracy 0.5048\n",
            "Epoch 8 Batch 4650 Loss 0.9235 Accuracy 0.5047\n",
            "Epoch 8 Batch 4700 Loss 0.9247 Accuracy 0.5045\n",
            "Epoch 8 Batch 4750 Loss 0.9261 Accuracy 0.5043\n",
            "Epoch 8 Batch 4800 Loss 0.9274 Accuracy 0.5042\n",
            "Epoch 8 Batch 4850 Loss 0.9287 Accuracy 0.5040\n",
            "Epoch 8 Batch 4900 Loss 0.9299 Accuracy 0.5038\n",
            "Epoch 8 Batch 4950 Loss 0.9312 Accuracy 0.5036\n",
            "Epoch 8 Batch 5000 Loss 0.9323 Accuracy 0.5034\n",
            "Epoch 8 Batch 5050 Loss 0.9335 Accuracy 0.5032\n",
            "Epoch 8 Batch 5100 Loss 0.9348 Accuracy 0.5029\n",
            "Epoch 8 Batch 5150 Loss 0.9360 Accuracy 0.5026\n",
            "Epoch 8 Batch 5200 Loss 0.9372 Accuracy 0.5024\n",
            "Epoch 8 Batch 5250 Loss 0.9384 Accuracy 0.5021\n",
            "Epoch 8 Batch 5300 Loss 0.9395 Accuracy 0.5018\n",
            "Epoch 8 Batch 5350 Loss 0.9408 Accuracy 0.5015\n",
            "Epoch 8 Batch 5400 Loss 0.9419 Accuracy 0.5012\n",
            "Epoch 8 Batch 5450 Loss 0.9430 Accuracy 0.5010\n",
            "Epoch 8 Batch 5500 Loss 0.9437 Accuracy 0.5007\n",
            "Epoch 8 Batch 5550 Loss 0.9448 Accuracy 0.5004\n",
            "Epoch 8 Batch 5600 Loss 0.9460 Accuracy 0.5001\n",
            "Epoch 8 Batch 5650 Loss 0.9470 Accuracy 0.4999\n",
            "Epoch 8 Batch 5700 Loss 0.9477 Accuracy 0.4997\n",
            "Saving checkpoint for epoch 8 at ./content/drive/My Drive/SelfPractise/DeepNLP/Transformer/ckpt/ckpt-8\n",
            "Time taken for 1 epoch: 1487.721866607666 secs\n",
            "\n",
            "Start of epoch 9\n",
            "Epoch 9 Batch 0 Loss 0.9598 Accuracy 0.4391\n",
            "Epoch 9 Batch 50 Loss 1.0520 Accuracy 0.4826\n",
            "Epoch 9 Batch 100 Loss 1.0522 Accuracy 0.4821\n",
            "Epoch 9 Batch 150 Loss 1.0485 Accuracy 0.4836\n",
            "Epoch 9 Batch 200 Loss 1.0490 Accuracy 0.4839\n",
            "Epoch 9 Batch 250 Loss 1.0431 Accuracy 0.4835\n",
            "Epoch 9 Batch 300 Loss 1.0398 Accuracy 0.4836\n",
            "Epoch 9 Batch 350 Loss 1.0375 Accuracy 0.4838\n",
            "Epoch 9 Batch 400 Loss 1.0342 Accuracy 0.4841\n",
            "Epoch 9 Batch 450 Loss 1.0330 Accuracy 0.4837\n",
            "Epoch 9 Batch 500 Loss 1.0318 Accuracy 0.4839\n",
            "Epoch 9 Batch 550 Loss 1.0328 Accuracy 0.4835\n",
            "Epoch 9 Batch 600 Loss 1.0297 Accuracy 0.4835\n",
            "Epoch 9 Batch 650 Loss 1.0286 Accuracy 0.4835\n",
            "Epoch 9 Batch 700 Loss 1.0284 Accuracy 0.4838\n",
            "Epoch 9 Batch 750 Loss 1.0272 Accuracy 0.4843\n",
            "Epoch 9 Batch 800 Loss 1.0266 Accuracy 0.4847\n",
            "Epoch 9 Batch 850 Loss 1.0263 Accuracy 0.4848\n",
            "Epoch 9 Batch 900 Loss 1.0255 Accuracy 0.4847\n",
            "Epoch 9 Batch 950 Loss 1.0242 Accuracy 0.4844\n",
            "Epoch 9 Batch 1000 Loss 1.0226 Accuracy 0.4845\n",
            "Epoch 9 Batch 1050 Loss 1.0217 Accuracy 0.4850\n",
            "Epoch 9 Batch 1100 Loss 1.0205 Accuracy 0.4851\n",
            "Epoch 9 Batch 1150 Loss 1.0193 Accuracy 0.4855\n",
            "Epoch 9 Batch 1200 Loss 1.0175 Accuracy 0.4857\n",
            "Epoch 9 Batch 1250 Loss 1.0163 Accuracy 0.4859\n",
            "Epoch 9 Batch 1300 Loss 1.0143 Accuracy 0.4863\n",
            "Epoch 9 Batch 1350 Loss 1.0118 Accuracy 0.4870\n",
            "Epoch 9 Batch 1400 Loss 1.0089 Accuracy 0.4875\n",
            "Epoch 9 Batch 1450 Loss 1.0061 Accuracy 0.4881\n",
            "Epoch 9 Batch 1500 Loss 1.0036 Accuracy 0.4889\n",
            "Epoch 9 Batch 1550 Loss 1.0015 Accuracy 0.4898\n",
            "Epoch 9 Batch 1600 Loss 0.9996 Accuracy 0.4907\n",
            "Epoch 9 Batch 1650 Loss 0.9977 Accuracy 0.4916\n",
            "Epoch 9 Batch 1700 Loss 0.9949 Accuracy 0.4924\n",
            "Epoch 9 Batch 1750 Loss 0.9926 Accuracy 0.4932\n",
            "Epoch 9 Batch 1800 Loss 0.9901 Accuracy 0.4940\n",
            "Epoch 9 Batch 1850 Loss 0.9875 Accuracy 0.4948\n",
            "Epoch 9 Batch 1900 Loss 0.9858 Accuracy 0.4957\n",
            "Epoch 9 Batch 1950 Loss 0.9836 Accuracy 0.4965\n",
            "Epoch 9 Batch 2000 Loss 0.9813 Accuracy 0.4971\n",
            "Epoch 9 Batch 2050 Loss 0.9791 Accuracy 0.4975\n",
            "Epoch 9 Batch 2100 Loss 0.9768 Accuracy 0.4979\n",
            "Epoch 9 Batch 2150 Loss 0.9744 Accuracy 0.4982\n",
            "Epoch 9 Batch 2200 Loss 0.9711 Accuracy 0.4984\n",
            "Epoch 9 Batch 2250 Loss 0.9677 Accuracy 0.4986\n",
            "Epoch 9 Batch 2300 Loss 0.9652 Accuracy 0.4987\n",
            "Epoch 9 Batch 2350 Loss 0.9625 Accuracy 0.4990\n",
            "Epoch 9 Batch 2400 Loss 0.9599 Accuracy 0.4992\n",
            "Epoch 9 Batch 2450 Loss 0.9574 Accuracy 0.4995\n",
            "Epoch 9 Batch 2500 Loss 0.9541 Accuracy 0.4997\n",
            "Epoch 9 Batch 2550 Loss 0.9515 Accuracy 0.5000\n",
            "Epoch 9 Batch 2600 Loss 0.9489 Accuracy 0.5003\n",
            "Epoch 9 Batch 2650 Loss 0.9467 Accuracy 0.5006\n",
            "Epoch 9 Batch 2700 Loss 0.9443 Accuracy 0.5008\n",
            "Epoch 9 Batch 2750 Loss 0.9418 Accuracy 0.5011\n",
            "Epoch 9 Batch 2800 Loss 0.9397 Accuracy 0.5014\n",
            "Epoch 9 Batch 2850 Loss 0.9376 Accuracy 0.5017\n",
            "Epoch 9 Batch 2900 Loss 0.9357 Accuracy 0.5020\n",
            "Epoch 9 Batch 2950 Loss 0.9341 Accuracy 0.5023\n",
            "Epoch 9 Batch 3000 Loss 0.9318 Accuracy 0.5025\n",
            "Epoch 9 Batch 3050 Loss 0.9301 Accuracy 0.5027\n",
            "Epoch 9 Batch 3100 Loss 0.9281 Accuracy 0.5031\n",
            "Epoch 9 Batch 3150 Loss 0.9263 Accuracy 0.5033\n",
            "Epoch 9 Batch 3200 Loss 0.9246 Accuracy 0.5035\n",
            "Epoch 9 Batch 3250 Loss 0.9230 Accuracy 0.5036\n",
            "Epoch 9 Batch 3300 Loss 0.9211 Accuracy 0.5039\n",
            "Epoch 9 Batch 3350 Loss 0.9191 Accuracy 0.5041\n",
            "Epoch 9 Batch 3400 Loss 0.9171 Accuracy 0.5044\n",
            "Epoch 9 Batch 3450 Loss 0.9151 Accuracy 0.5047\n",
            "Epoch 9 Batch 3500 Loss 0.9133 Accuracy 0.5050\n",
            "Epoch 9 Batch 3550 Loss 0.9112 Accuracy 0.5053\n",
            "Epoch 9 Batch 3600 Loss 0.9095 Accuracy 0.5056\n",
            "Epoch 9 Batch 3650 Loss 0.9081 Accuracy 0.5060\n",
            "Epoch 9 Batch 3700 Loss 0.9063 Accuracy 0.5063\n",
            "Epoch 9 Batch 3750 Loss 0.9050 Accuracy 0.5066\n",
            "Epoch 9 Batch 3800 Loss 0.9036 Accuracy 0.5069\n",
            "Epoch 9 Batch 3850 Loss 0.9021 Accuracy 0.5072\n",
            "Epoch 9 Batch 3900 Loss 0.9006 Accuracy 0.5076\n",
            "Epoch 9 Batch 3950 Loss 0.8993 Accuracy 0.5080\n",
            "Epoch 9 Batch 4000 Loss 0.8979 Accuracy 0.5084\n",
            "Epoch 9 Batch 4050 Loss 0.8968 Accuracy 0.5087\n",
            "Epoch 9 Batch 4100 Loss 0.8961 Accuracy 0.5089\n",
            "Epoch 9 Batch 4150 Loss 0.8958 Accuracy 0.5089\n",
            "Epoch 9 Batch 4200 Loss 0.8958 Accuracy 0.5090\n",
            "Epoch 9 Batch 4250 Loss 0.8963 Accuracy 0.5089\n",
            "Epoch 9 Batch 4300 Loss 0.8971 Accuracy 0.5088\n",
            "Epoch 9 Batch 4350 Loss 0.8982 Accuracy 0.5087\n",
            "Epoch 9 Batch 4400 Loss 0.8993 Accuracy 0.5085\n",
            "Epoch 9 Batch 4450 Loss 0.9006 Accuracy 0.5084\n",
            "Epoch 9 Batch 4500 Loss 0.9018 Accuracy 0.5082\n",
            "Epoch 9 Batch 4550 Loss 0.9030 Accuracy 0.5080\n",
            "Epoch 9 Batch 4600 Loss 0.9044 Accuracy 0.5077\n",
            "Epoch 9 Batch 4650 Loss 0.9059 Accuracy 0.5075\n",
            "Epoch 9 Batch 4700 Loss 0.9076 Accuracy 0.5073\n",
            "Epoch 9 Batch 4750 Loss 0.9087 Accuracy 0.5071\n",
            "Epoch 9 Batch 4800 Loss 0.9098 Accuracy 0.5069\n",
            "Epoch 9 Batch 4850 Loss 0.9112 Accuracy 0.5067\n",
            "Epoch 9 Batch 4900 Loss 0.9125 Accuracy 0.5065\n",
            "Epoch 9 Batch 4950 Loss 0.9137 Accuracy 0.5063\n",
            "Epoch 9 Batch 5000 Loss 0.9151 Accuracy 0.5060\n",
            "Epoch 9 Batch 5050 Loss 0.9165 Accuracy 0.5058\n",
            "Epoch 9 Batch 5100 Loss 0.9176 Accuracy 0.5056\n",
            "Epoch 9 Batch 5150 Loss 0.9191 Accuracy 0.5054\n",
            "Epoch 9 Batch 5200 Loss 0.9203 Accuracy 0.5051\n",
            "Epoch 9 Batch 5250 Loss 0.9213 Accuracy 0.5048\n",
            "Epoch 9 Batch 5300 Loss 0.9225 Accuracy 0.5045\n",
            "Epoch 9 Batch 5350 Loss 0.9237 Accuracy 0.5043\n",
            "Epoch 9 Batch 5400 Loss 0.9247 Accuracy 0.5040\n",
            "Epoch 9 Batch 5450 Loss 0.9258 Accuracy 0.5037\n",
            "Epoch 9 Batch 5500 Loss 0.9268 Accuracy 0.5035\n",
            "Epoch 9 Batch 5550 Loss 0.9277 Accuracy 0.5032\n",
            "Epoch 9 Batch 5600 Loss 0.9288 Accuracy 0.5030\n",
            "Epoch 9 Batch 5650 Loss 0.9298 Accuracy 0.5028\n",
            "Epoch 9 Batch 5700 Loss 0.9308 Accuracy 0.5025\n",
            "Saving checkpoint for epoch 9 at ./content/drive/My Drive/SelfPractise/DeepNLP/Transformer/ckpt/ckpt-9\n",
            "Time taken for 1 epoch: 1444.9338698387146 secs\n",
            "\n",
            "Start of epoch 10\n",
            "Epoch 10 Batch 0 Loss 1.0933 Accuracy 0.4638\n",
            "Epoch 10 Batch 50 Loss 1.0168 Accuracy 0.4855\n",
            "Epoch 10 Batch 100 Loss 1.0314 Accuracy 0.4849\n",
            "Epoch 10 Batch 150 Loss 1.0288 Accuracy 0.4847\n",
            "Epoch 10 Batch 200 Loss 1.0292 Accuracy 0.4843\n",
            "Epoch 10 Batch 250 Loss 1.0239 Accuracy 0.4856\n",
            "Epoch 10 Batch 300 Loss 1.0195 Accuracy 0.4862\n",
            "Epoch 10 Batch 350 Loss 1.0214 Accuracy 0.4864\n",
            "Epoch 10 Batch 400 Loss 1.0209 Accuracy 0.4860\n",
            "Epoch 10 Batch 450 Loss 1.0183 Accuracy 0.4861\n",
            "Epoch 10 Batch 500 Loss 1.0168 Accuracy 0.4856\n",
            "Epoch 10 Batch 550 Loss 1.0152 Accuracy 0.4858\n",
            "Epoch 10 Batch 600 Loss 1.0142 Accuracy 0.4862\n",
            "Epoch 10 Batch 650 Loss 1.0149 Accuracy 0.4862\n",
            "Epoch 10 Batch 700 Loss 1.0141 Accuracy 0.4863\n",
            "Epoch 10 Batch 750 Loss 1.0137 Accuracy 0.4867\n",
            "Epoch 10 Batch 800 Loss 1.0138 Accuracy 0.4871\n",
            "Epoch 10 Batch 850 Loss 1.0129 Accuracy 0.4870\n",
            "Epoch 10 Batch 900 Loss 1.0103 Accuracy 0.4873\n",
            "Epoch 10 Batch 950 Loss 1.0099 Accuracy 0.4875\n",
            "Epoch 10 Batch 1000 Loss 1.0081 Accuracy 0.4875\n",
            "Epoch 10 Batch 1050 Loss 1.0066 Accuracy 0.4878\n",
            "Epoch 10 Batch 1100 Loss 1.0048 Accuracy 0.4877\n",
            "Epoch 10 Batch 1150 Loss 1.0028 Accuracy 0.4878\n",
            "Epoch 10 Batch 1200 Loss 1.0016 Accuracy 0.4880\n",
            "Epoch 10 Batch 1250 Loss 1.0004 Accuracy 0.4885\n",
            "Epoch 10 Batch 1300 Loss 0.9986 Accuracy 0.4888\n",
            "Epoch 10 Batch 1350 Loss 0.9965 Accuracy 0.4896\n",
            "Epoch 10 Batch 1400 Loss 0.9946 Accuracy 0.4902\n",
            "Epoch 10 Batch 1450 Loss 0.9921 Accuracy 0.4911\n",
            "Epoch 10 Batch 1500 Loss 0.9892 Accuracy 0.4918\n",
            "Epoch 10 Batch 1550 Loss 0.9869 Accuracy 0.4927\n",
            "Epoch 10 Batch 1600 Loss 0.9845 Accuracy 0.4935\n",
            "Epoch 10 Batch 1650 Loss 0.9819 Accuracy 0.4944\n",
            "Epoch 10 Batch 1700 Loss 0.9789 Accuracy 0.4952\n",
            "Epoch 10 Batch 1750 Loss 0.9761 Accuracy 0.4962\n",
            "Epoch 10 Batch 1800 Loss 0.9746 Accuracy 0.4969\n",
            "Epoch 10 Batch 1850 Loss 0.9721 Accuracy 0.4975\n",
            "Epoch 10 Batch 1900 Loss 0.9697 Accuracy 0.4982\n",
            "Epoch 10 Batch 1950 Loss 0.9676 Accuracy 0.4991\n",
            "Epoch 10 Batch 2000 Loss 0.9660 Accuracy 0.4997\n",
            "Epoch 10 Batch 2050 Loss 0.9637 Accuracy 0.5002\n",
            "Epoch 10 Batch 2100 Loss 0.9618 Accuracy 0.5005\n",
            "Epoch 10 Batch 2150 Loss 0.9594 Accuracy 0.5007\n",
            "Epoch 10 Batch 2200 Loss 0.9566 Accuracy 0.5009\n",
            "Epoch 10 Batch 2250 Loss 0.9534 Accuracy 0.5011\n",
            "Epoch 10 Batch 2300 Loss 0.9508 Accuracy 0.5012\n",
            "Epoch 10 Batch 2350 Loss 0.9480 Accuracy 0.5013\n",
            "Epoch 10 Batch 2400 Loss 0.9454 Accuracy 0.5016\n",
            "Epoch 10 Batch 2450 Loss 0.9420 Accuracy 0.5019\n",
            "Epoch 10 Batch 2500 Loss 0.9394 Accuracy 0.5021\n",
            "Epoch 10 Batch 2550 Loss 0.9365 Accuracy 0.5024\n",
            "Epoch 10 Batch 2600 Loss 0.9339 Accuracy 0.5027\n",
            "Epoch 10 Batch 2650 Loss 0.9318 Accuracy 0.5031\n",
            "Epoch 10 Batch 2700 Loss 0.9295 Accuracy 0.5034\n",
            "Epoch 10 Batch 2750 Loss 0.9271 Accuracy 0.5038\n",
            "Epoch 10 Batch 2800 Loss 0.9250 Accuracy 0.5041\n",
            "Epoch 10 Batch 2850 Loss 0.9231 Accuracy 0.5042\n",
            "Epoch 10 Batch 2900 Loss 0.9212 Accuracy 0.5045\n",
            "Epoch 10 Batch 2950 Loss 0.9192 Accuracy 0.5048\n",
            "Epoch 10 Batch 3000 Loss 0.9174 Accuracy 0.5051\n",
            "Epoch 10 Batch 3050 Loss 0.9158 Accuracy 0.5053\n",
            "Epoch 10 Batch 3100 Loss 0.9137 Accuracy 0.5056\n",
            "Epoch 10 Batch 3150 Loss 0.9119 Accuracy 0.5059\n",
            "Epoch 10 Batch 3200 Loss 0.9102 Accuracy 0.5060\n",
            "Epoch 10 Batch 3250 Loss 0.9082 Accuracy 0.5063\n",
            "Epoch 10 Batch 3300 Loss 0.9062 Accuracy 0.5064\n",
            "Epoch 10 Batch 3350 Loss 0.9042 Accuracy 0.5067\n",
            "Epoch 10 Batch 3400 Loss 0.9024 Accuracy 0.5069\n",
            "Epoch 10 Batch 3450 Loss 0.9008 Accuracy 0.5073\n",
            "Epoch 10 Batch 3500 Loss 0.8989 Accuracy 0.5075\n",
            "Epoch 10 Batch 3550 Loss 0.8975 Accuracy 0.5078\n",
            "Epoch 10 Batch 3600 Loss 0.8959 Accuracy 0.5082\n",
            "Epoch 10 Batch 3650 Loss 0.8941 Accuracy 0.5084\n",
            "Epoch 10 Batch 3700 Loss 0.8925 Accuracy 0.5087\n",
            "Epoch 10 Batch 3750 Loss 0.8913 Accuracy 0.5091\n",
            "Epoch 10 Batch 3800 Loss 0.8897 Accuracy 0.5094\n",
            "Epoch 10 Batch 3850 Loss 0.8880 Accuracy 0.5098\n",
            "Epoch 10 Batch 3900 Loss 0.8865 Accuracy 0.5101\n",
            "Epoch 10 Batch 3950 Loss 0.8852 Accuracy 0.5104\n",
            "Epoch 10 Batch 4000 Loss 0.8840 Accuracy 0.5107\n",
            "Epoch 10 Batch 4050 Loss 0.8828 Accuracy 0.5109\n",
            "Epoch 10 Batch 4100 Loss 0.8820 Accuracy 0.5110\n",
            "Epoch 10 Batch 4150 Loss 0.8816 Accuracy 0.5111\n",
            "Epoch 10 Batch 4200 Loss 0.8817 Accuracy 0.5111\n",
            "Epoch 10 Batch 4250 Loss 0.8820 Accuracy 0.5111\n",
            "Epoch 10 Batch 4300 Loss 0.8827 Accuracy 0.5110\n",
            "Epoch 10 Batch 4350 Loss 0.8838 Accuracy 0.5108\n",
            "Epoch 10 Batch 4400 Loss 0.8846 Accuracy 0.5107\n",
            "Epoch 10 Batch 4450 Loss 0.8860 Accuracy 0.5105\n",
            "Epoch 10 Batch 4500 Loss 0.8872 Accuracy 0.5104\n",
            "Epoch 10 Batch 4550 Loss 0.8888 Accuracy 0.5101\n",
            "Epoch 10 Batch 4600 Loss 0.8904 Accuracy 0.5100\n",
            "Epoch 10 Batch 4650 Loss 0.8918 Accuracy 0.5098\n",
            "Epoch 10 Batch 4700 Loss 0.8931 Accuracy 0.5095\n",
            "Epoch 10 Batch 4750 Loss 0.8944 Accuracy 0.5094\n",
            "Epoch 10 Batch 4800 Loss 0.8955 Accuracy 0.5093\n",
            "Epoch 10 Batch 4850 Loss 0.8967 Accuracy 0.5092\n",
            "Epoch 10 Batch 4900 Loss 0.8980 Accuracy 0.5090\n",
            "Epoch 10 Batch 4950 Loss 0.8994 Accuracy 0.5087\n",
            "Epoch 10 Batch 5000 Loss 0.9007 Accuracy 0.5084\n",
            "Epoch 10 Batch 5050 Loss 0.9022 Accuracy 0.5082\n",
            "Epoch 10 Batch 5100 Loss 0.9037 Accuracy 0.5080\n",
            "Epoch 10 Batch 5150 Loss 0.9049 Accuracy 0.5077\n",
            "Epoch 10 Batch 5200 Loss 0.9061 Accuracy 0.5074\n",
            "Epoch 10 Batch 5250 Loss 0.9072 Accuracy 0.5071\n",
            "Epoch 10 Batch 5300 Loss 0.9083 Accuracy 0.5068\n",
            "Epoch 10 Batch 5350 Loss 0.9093 Accuracy 0.5065\n",
            "Epoch 10 Batch 5400 Loss 0.9104 Accuracy 0.5062\n",
            "Epoch 10 Batch 5450 Loss 0.9116 Accuracy 0.5060\n",
            "Epoch 10 Batch 5500 Loss 0.9126 Accuracy 0.5057\n",
            "Epoch 10 Batch 5550 Loss 0.9137 Accuracy 0.5055\n",
            "Epoch 10 Batch 5600 Loss 0.9147 Accuracy 0.5052\n",
            "Epoch 10 Batch 5650 Loss 0.9155 Accuracy 0.5050\n",
            "Epoch 10 Batch 5700 Loss 0.9163 Accuracy 0.5048\n",
            "Saving checkpoint for epoch 10 at ./content/drive/My Drive/SelfPractise/DeepNLP/Transformer/ckpt/ckpt-10\n",
            "Time taken for 1 epoch: 1445.6679577827454 secs\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Hd9CffIDBiS",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgLPau9k_RvO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(inp_sentence):\n",
        "    inp_sentence = \\\n",
        "        [VOCAB_SIZE_EN-2] + tokenizer_en.encode(inp_sentence) + [VOCAB_SIZE_EN-1]\n",
        "    enc_input = tf.expand_dims(inp_sentence, axis=0)\n",
        "    \n",
        "    output = tf.expand_dims([VOCAB_SIZE_FR-2], axis=0)\n",
        "    \n",
        "    for _ in range(MAX_LENGTH):\n",
        "        predictions = transformer(enc_input, output, False)\n",
        "        \n",
        "        prediction = predictions[:, -1:, :]\n",
        "        \n",
        "        predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.int32)\n",
        "        \n",
        "        if predicted_id == VOCAB_SIZE_FR-1:\n",
        "            return tf.squeeze(output, axis=0)\n",
        "        \n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "        \n",
        "    return tf.squeeze(output, axis=0)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2vHOvZCEXQe",
        "colab_type": "text"
      },
      "source": [
        "# Translation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrLp3LDPETXq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(sentence):\n",
        "    output = evaluate(sentence).numpy()\n",
        "    \n",
        "    predicted_sentence = tokenizer_fr.decode(\n",
        "        [i for i in output if i < VOCAB_SIZE_FR-2]\n",
        "    )\n",
        "    \n",
        "    print(\"Input: {}\".format(sentence))\n",
        "    print(\"Predicted translation: {}\".format(predicted_sentence))"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEzBUu4nEatG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "23ec572a-9861-41b0-b131-a01c59142ce0"
      },
      "source": [
        "translate(\"This is a really powerful tool!\")"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: This is a really powerful tool!\n",
            "Predicted translation: C'est un instrument vraiment fort!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyKPmHz8Erri",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f1ddcd6c-2aea-4c18-b4ec-a3c3d28c48a2"
      },
      "source": [
        "translate(\"She is very kind human being\")"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: She is very kind human being\n",
            "Predicted translation: Elle est très importante\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJYVKC67EdzQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2c27de01-25c6-4db3-d3af-32fbeba73ccd"
      },
      "source": [
        "translate(\"My Heart beats for her, why?\")"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: My Heart beats for her, why?\n",
            "Predicted translation: Ma belle abstraction est-elle la bonne?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BN7vjOWSGYzq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "258c2b72-5909-46b9-9637-535b17963150"
      },
      "source": [
        "translate(\"I am a man.\")"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: I am a man.\n",
            "Predicted translation: Je suis un homme.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvgY7DNHGszc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c488d976-6d58-4f8a-b7ac-f6c52dfe4a41"
      },
      "source": [
        "translate(\"I am in the space.\")"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: I am in the space.\n",
            "Predicted translation: Je suis dans l'espace de l'affaire.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdWn4wgjHpZs",
        "colab_type": "text"
      },
      "source": [
        "I ran this only for 10 Epochs and received accuracy of 50%. As per the accuracy It seems to be working fine. But If we have High end GPU machines. We can train the same model with many number of epochs and get a high accuracy. \n",
        "\n",
        "\n",
        "Observations :\n",
        "\n",
        "1. After evaluating in on different sentences, I found word sequence is maintained but meaning of the words are sometime getting different , Probably it is because of the low accuracy.\n",
        "\n",
        "2. If remove the full stop from the sentence, It gives different words sometime.\n",
        "so here We can these symbolls are playing role in terms of balancing the translated sentences\n",
        "\n",
        "3. If our aim is to input long sentences, We need to make sure vocab size does handles the number of words in the sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIPiIBxoJZjv",
        "colab_type": "text"
      },
      "source": [
        "# Challenges faced in doing this Project\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmhDXtKfJhx7",
        "colab_type": "text"
      },
      "source": [
        "Challenges :\n",
        "\n",
        "\n",
        "For the first time, I got good accuracy but transformer's result was not satisfying. Whatever input i used to give to the transformers. It kept giving the sentence with same words.\n",
        "\n",
        "Later I found out that it is because in Scale.product forgot the minus sign that was not supposed to there hence what is did was \n",
        "\n",
        "It multiplied the mask by zero instead of minus infinity so Look ahead mask which i built was not applied,  and Decoder allowed itself to have a look at  the future words instead of guessing them\n"
      ]
    }
  ]
}